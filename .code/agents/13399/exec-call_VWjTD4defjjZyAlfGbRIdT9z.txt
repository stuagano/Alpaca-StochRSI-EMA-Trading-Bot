🎯 ADK auto-sync aliases loaded!
💡 Use 'sync' to sync with upstream anytime
💡 Use 'synccheck' to check sync status
./.code/agents/1bc6f74c-130b-43a9-909e-90ff966275df/result.txt:318:test_refresh_scanner_symbols.py
./.code/agents/1bc6f74c-130b-43a9-909e-90ff966275df/result.txt:325:[2025-10-14T02:48:03] exec /bin/bash -lc "sed -n '1,200p' tests/unit/test_refresh_scanner_symbols.py" in /Users/stuartgano/Desktop/Penny Personal Assistant/Alpaca-StochRSI-EMA-Trading-Bot
./.code/agents/1bc6f74c-130b-43a9-909e-90ff966275df/result.txt:326:[2025-10-14T02:48:04] /bin/bash -lc "sed -n '1,200p' tests/unit/test_refresh_scanner_symbols.py" succeeded in 774ms:
./.code/agents/1bc6f74c-130b-43a9-909e-90ff966275df/result.txt:330:"""Unit tests for ``refresh_scanner_symbols`` coordination logic."""
./.code/agents/1bc6f74c-130b-43a9-909e-90ff966275df/result.txt:1130:[2025-10-14T02:51:23] exec /bin/bash -lc 'nl -ba tests/unit/test_refresh_scanner_symbols.py' in /Users/stuartgano/Desktop/Penny Personal Assistant/Alpaca-StochRSI-EMA-Trading-Bot
./.code/agents/1bc6f74c-130b-43a9-909e-90ff966275df/result.txt:1131:[2025-10-14T02:51:24] /bin/bash -lc 'nl -ba tests/unit/test_refresh_scanner_symbols.py' succeeded in 696ms:
./.code/agents/1bc6f74c-130b-43a9-909e-90ff966275df/result.txt:1135:     1	"""Unit tests for ``refresh_scanner_symbols`` coordination logic."""
./.code/agents/1bc6f74c-130b-43a9-909e-90ff966275df/result.txt:1218:2. Repair the crypto execution pipeline: Rework `backend/api/services/trading_service.py:30-158` so it constructs `SignalProcessor` with a live `TradingExecutor` (see expectation in `signal_processor.py:18-70`), migrates off `alpaca_trade_api` to the `alpaca-py` clients already used in `main.py:99-135`, and shares the `CryptoDayTradingBot`/scanner state. That brings the dashboard APIs in line with the production crypto flow and eliminates silent failures in signal execution.
./.code/agents/1bc6f74c-130b-43a9-909e-90ff966275df/result.txt:1220:3. Harden the test suite around the new architecture: Stop importing archived modules (`tests/unit/test_refresh_scanner_symbols.py:20`) by porting the tests to `services/unified_trading/state.py`, add Flask test-client coverage for the blueprint endpoints, and refresh the documentation in `COMPLETE-TESTING-SUMMARY.md` and `FUNCTIONAL-TESTING-GUIDE.md` to match the actual test files. This prevents regressions slipping through and aligns the guides with what contributors can really run.
./.code/agents/14dadd09-8e54-4499-a9f0-452c2631a248/result.txt:22:   - Replace the heavy monkey-patching smoke test with focused unit tests that exercise `create_crypto_day_trader`, scanner selection, and risk controls using shared fixtures, then add integration tests that mock Alpaca endpoints to validate order/position flows without real API calls.
./.code/agents/14dadd09-8e54-4499-a9f0-452c2631a248/result.txt:27:   - Instrument trade execution, scanner health, and risk decisions with structured metrics/logs rather than only tracking startup/exceptions; expose them via the Prometheus exporter that `main.py` already bootstraps so Grafana dashboards become actionable.
./.code/agents/14dadd09-8e54-4499-a9f0-452c2631a248/result.txt:116:51. I'll outline some steps: centralize configuration, refactor scanner, improve tests, and add better metrics.
./.code/agents/14dadd09-8e54-4499-a9f0-452c2631a248/result.txt:139:   - Replace the heavy monkey-patching smoke test with focused unit tests that exercise `create_crypto_day_trader`, scanner selection, and risk controls using shared fixtures, then add integration tests that mock Alpaca endpoints to validate order/position flows without real API calls.【F:tests/functional/test_crypto_scalping_main.py†L1-L106】【F:tests/functional/test_crypto_config.py†L1-L22】  
./.code/agents/14dadd09-8e54-4499-a9f0-452c2631a248/result.txt:143:   - Instrument trade execution, scanner health, and risk decisions with structured metrics/logs rather than only tracking startup/exceptions; expose them via the Prometheus exporter that `main.py` already bootstraps so Grafana dashboards become actionable.【F:main.py†L42-L65】【F:main.py†L200-L208】【F:docs/local_runtime.md†L43-L63】  
./archive/legacy_2025Q4/crypto_bot_with_dashboard.py:283:        crypto_bot.scanner.update_enabled_symbols(config.symbols)
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:35:    refresh_scanner_symbols as sync_scanner_symbols,
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:38:    to_scanner_symbol,
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:58:def refresh_scanner_symbols(state: TradingState) -> List[str]:
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:59:    """Synchronise cached scanner symbols with the active strategy."""
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:61:    return sync_scanner_symbols(state)
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:213:async def crypto_scanner(state: TradingState):
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:214:    """Lightweight crypto scanner driven by configured interval."""
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:217:            crypto_pairs = refresh_scanner_symbols(state)
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:220:                await asyncio.sleep(state.settings.crypto_scanner_interval_seconds)
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:239:                            'symbol': to_scanner_symbol(symbol),
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:259:        await asyncio.sleep(state.settings.crypto_scanner_interval_seconds)
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:280:            # Update scanner with current market data
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:281:            await update_scanner_data(state)
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:302:async def update_scanner_data(state: TradingState):
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:303:    """Update the crypto volatility scanner with current market data"""
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:305:        if not state.crypto_scanner:
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:306:            logger.debug("No crypto scanner available for data update")
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:309:        logger.debug("Updating scanner with market data...")
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:313:        # Update market data for all configured scanner symbols
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:314:        for symbol_display in refresh_scanner_symbols(state):
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:319:                    symbol_clean = to_scanner_symbol(symbol_display)
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:324:                    # Update scanner data
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:325:                    state.crypto_scanner.update_market_data(symbol_clean, price, volume)
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:334:    """Find new scalping entry opportunities using the volatility scanner"""
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:336:        if not state.crypto_scanner:
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:337:            logger.debug("No crypto scanner available")
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:340:        # Get trading signals from the scanner
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:341:        signals = state.crypto_scanner.scan_for_opportunities()
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:360:            # Note: confidence values from scanner are 0-1 scale (0.5 = 50% confidence)
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:556:        # Initialize crypto volatility scanner from configured symbols
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:557:        scanner_seed_symbols = [
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:558:            to_scanner_symbol(symbol)
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:562:        state.crypto_scanner = CryptoVolatilityScanner(
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:563:            enabled_symbols=scanner_seed_symbols or None
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:565:        derived_pairs = refresh_scanner_symbols(state)
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:712:        "last_scan": state.last_update.get('scanner', datetime.now(timezone.utc)).isoformat(),
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:722:        if state.crypto_scanner:
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:723:            # Get signals from the volatility scanner
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:724:            scanner_signals = state.crypto_scanner.scan_for_opportunities()
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:726:            for signal in scanner_signals[:5]:  # Top 5 signals
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:738:                    "data_source": "scalping_scanner"
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:741:        # Fallback to simple signals if scanner not available
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:764:        if state.crypto_scanner:
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:765:            # Convert symbol format for scanner (BTC/USD -> BTCUSD)
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:766:            scanner_symbol = symbol.replace('/', '').upper()
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:767:            if not scanner_symbol.endswith('USD'):
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:768:                scanner_symbol += 'USD'
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:771:            all_signals = state.crypto_scanner.scan_for_opportunities()
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:774:                if signal.symbol == scanner_symbol:
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:787:                        "data_source": "scalping_scanner"
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:917:        "scalping_enabled": state.crypto_scanner is not None,
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:924:        "scanner_symbols": len(state.crypto_scanner.get_enabled_symbols()) if state.crypto_scanner else 0
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:932:        "scalping_enabled": state.crypto_scanner is not None,
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:951:    """Generate a specific scalping signal for a symbol using the scanner"""
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:953:        if not state.crypto_scanner:
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:954:            return {"error": "Scalping scanner not initialized", "data_source": "error"}
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:957:        scanner_symbol = symbol.replace('/', '').upper()
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:958:        if not scanner_symbol.endswith('USD'):
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:959:            scanner_symbol += 'USD'
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:962:        if scanner_symbol not in state.crypto_scanner.price_data:
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:967:                "data_source": "scanner"
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:970:        prices = state.crypto_scanner.price_data[scanner_symbol]
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:971:        volumes = state.crypto_scanner.volume_data.get(scanner_symbol, [])
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:978:                "data_source": "scanner"
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:983:        volatility = state.crypto_scanner.calculate_volatility(prices)
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:984:        volume_surge = state.crypto_scanner.detect_volume_surge(volumes)
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:985:        momentum = state.crypto_scanner.calculate_momentum(prices)
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:987:        # Generate signal using the scanner's internal method
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:988:        signal = state.crypto_scanner._generate_signal(
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:989:            scanner_symbol, current_price, volatility, volume_surge, momentum
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:1012:                "data_source": "scalping_scanner"
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:1027:                "data_source": "scalping_scanner"
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:1068:            "scanner_enabled": state.crypto_scanner is not None,
./archive/legacy_2025Q4/unified_trading_service_with_frontend.py:1069:            "enabled_symbols": state.crypto_scanner.get_enabled_symbols() if state.crypto_scanner else [],
./config/service_settings.py:86:    crypto_scanner_interval_seconds: int
./config/service_settings.py:252:    crypto_scanner_interval = _parse_int(
./config/service_settings.py:263:        crypto_scanner_interval_seconds=crypto_scanner_interval,
Binary file ./strategies/__pycache__/crypto_scalping_strategy.cpython-313.pyc matches
Binary file ./strategies/__pycache__/crypto_scalping_strategy.cpython-312.pyc matches
./strategies/crypto_scalping_strategy.py:472:        self.scanner = CryptoVolatilityScanner()
./strategies/crypto_scalping_strategy.py:548:        signals = self.scanner.scan_for_opportunities()
./strategies/crypto_scalping_strategy.py:894:            with self.scanner.lock:
./strategies/crypto_scalping_strategy.py:895:                if symbol in self.scanner.price_data and self.scanner.price_data[symbol]:
./strategies/crypto_scalping_strategy.py:896:                    return self.scanner.price_data[symbol][-1]
./strategies/crypto_scalping_strategy.py:910:            for symbol in self.scanner.high_volume_pairs:
./strategies/crypto_scalping_strategy.py:925:                    self.scanner.update_market_data(symbol, price, volume)
./strategies/crypto_scalping_strategy.py:929:                    for symbol in self.scanner.high_volume_pairs:
./strategies/crypto_scalping_strategy.py:942:                        self.scanner.update_market_data(symbol, base_prices[symbol], volume)
./tests/unit/config/test_service_settings.py:64:    assert settings.crypto_scanner_interval_seconds == DEFAULT_SCANNER_INTERVAL_SECONDS
./tests/unit/config/test_service_settings.py:117:    assert settings.crypto_scanner_interval_seconds == 23
./tests/unit/test_refresh_scanner_symbols.py:1:"""Unit tests for ``refresh_scanner_symbols`` coordination logic."""
./tests/unit/test_refresh_scanner_symbols.py:25:    """Ensure tests do not leak scanner state between runs."""
./tests/unit/test_refresh_scanner_symbols.py:27:    original_scanner = uts_module.trading_state.crypto_scanner
./tests/unit/test_refresh_scanner_symbols.py:28:    original_symbols = list(uts_module.trading_state.crypto_scanner_symbols)
./tests/unit/test_refresh_scanner_symbols.py:32:        uts_module.trading_state.crypto_scanner = original_scanner
./tests/unit/test_refresh_scanner_symbols.py:33:        uts_module.trading_state.crypto_scanner_symbols = original_symbols
./tests/unit/test_refresh_scanner_symbols.py:36:def test_refresh_scanner_symbols_returns_cached_when_scanner_missing(uts_module):
./tests/unit/test_refresh_scanner_symbols.py:37:    """If the scanner is not initialised, cached symbols should be returned."""
./tests/unit/test_refresh_scanner_symbols.py:40:    uts_module.trading_state.crypto_scanner = None
./tests/unit/test_refresh_scanner_symbols.py:41:    uts_module.trading_state.crypto_scanner_symbols = list(expected)
./tests/unit/test_refresh_scanner_symbols.py:43:    assert uts_module.refresh_scanner_symbols() == expected
./tests/unit/test_refresh_scanner_symbols.py:47:def crypto_scanner_cls():
./tests/unit/test_refresh_scanner_symbols.py:53:def test_refresh_scanner_symbols_projects_strategy_output_to_display_list(
./tests/unit/test_refresh_scanner_symbols.py:54:    crypto_scanner_cls,
./tests/unit/test_refresh_scanner_symbols.py:59:    uts_module.trading_state.crypto_scanner = crypto_scanner_cls(enabled_symbols=[
./tests/unit/test_refresh_scanner_symbols.py:66:    derived = uts_module.refresh_scanner_symbols()
./tests/unit/test_refresh_scanner_symbols.py:69:    assert uts_module.trading_state.crypto_scanner_symbols == derived
./tests/unit/test_refresh_scanner_symbols.py:72:def test_refresh_scanner_symbols_clears_cache_when_strategy_returns_empty(
./tests/unit/test_refresh_scanner_symbols.py:73:    crypto_scanner_cls,
./tests/unit/test_refresh_scanner_symbols.py:78:    scanner = crypto_scanner_cls(enabled_symbols=["BTCUSD"])
./tests/unit/test_refresh_scanner_symbols.py:79:    scanner.update_enabled_symbols([])
./tests/unit/test_refresh_scanner_symbols.py:80:    uts_module.trading_state.crypto_scanner = scanner
./tests/unit/test_refresh_scanner_symbols.py:81:    uts_module.trading_state.crypto_scanner_symbols = ["BTC/USD"]
./tests/unit/test_refresh_scanner_symbols.py:83:    assert uts_module.refresh_scanner_symbols() == []
./tests/unit/test_refresh_scanner_symbols.py:84:    assert uts_module.trading_state.crypto_scanner_symbols == []
./tests/unit/services/unified_trading/test_background_workers.py:45:        "crypto_scanner",
./tests/helpers/dependency_checks.py:12:# Keep optional runtime dependencies that power the live crypto scanner in one place
./tests/functional/test_crypto_scalping_main.py:61:        self.scanner = _FakeScanner()
./tests/functional/test_crypto_scalping_main.py:106:    assert fake_bot.scanner.updated_symbols == config.symbols
Binary file ./tests/functional/__pycache__/test_crypto_scalping_main.cpython-313-pytest-8.4.2.pyc matches
Binary file ./tests/functional/__pycache__/test_crypto_scalping_main.cpython-313-pytest-8.4.1.pyc matches
Binary file ./__pycache__/main.cpython-313.pyc matches
./docs/local_runtime.md:31:TRADING_SERVICE_BACKGROUND_WORKERS=update_cache,crypto_scanner,crypto_scalping_trader
./README.md:27:   # Optional: install strategy runtime extras to exercise scanner-dependent tests
./README.md:56:- **.env**: Copy from `.env.example` to declare API credentials, scanner cadence, and runtime guardrails. Scanner symbols are derived from the strategy defaults and automatically sync with the values declared in `TRADING_SERVICE_CRYPTO_SYMBOLS`.
./.venv/lib/python3.13/site-packages/pip-25.2.dist-info/RECORD:500:pip/_vendor/pygments/__pycache__/scanner.cpython-313.pyc,,
./.venv/lib/python3.13/site-packages/pip-25.2.dist-info/RECORD:525:pip/_vendor/pygments/scanner.py,sha256=nNcETRR1tRuiTaHmHSTTECVYFPcLf6mDZu1e4u91A9E,3092
./.venv/lib/python3.13/site-packages/pyyaml-6.0.1.dist-info/RECORD:25:yaml/__pycache__/scanner.cpython-313.pyc,,
./.venv/lib/python3.13/site-packages/pyyaml-6.0.1.dist-info/RECORD:41:yaml/scanner.py,sha256=YEM3iLZSaQwXcQRg2l2R4MdT0zGP2F9eHkKGKnHyWQY,51279
./.venv/lib/python3.13/site-packages/pygments/scanner.py:2:    pygments.scanner
./.venv/lib/python3.13/site-packages/pygments/scanner.py:5:    This library implements a regex based scanner. Some languages
./.venv/lib/python3.13/site-packages/pygments/scanner.py:12:    this scanner.
./.venv/lib/python3.13/site-packages/pygments/scanner.py:29:    Simple scanner
./.venv/lib/python3.13/site-packages/pygments/scanner.py:50:        """`True` if the scanner reached the end of text."""
Binary file ./.venv/lib/python3.13/site-packages/pygments/lexers/__pycache__/pascal.cpython-313.pyc matches
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:17:from pygments.scanner import Scanner
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:423:        scanner = Scanner(text, re.DOTALL | re.MULTILINE | re.IGNORECASE)
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:434:        while not scanner.eos:
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:438:                if scanner.scan(r'\s+'):
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:440:                elif not self.is_portugol and scanner.scan(r'\{.*?\}|\(\*.*?\*\)'):
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:441:                    if scanner.match.startswith('$'):
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:445:                elif scanner.scan(r'//.*?$'):
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:447:                elif self.is_portugol and scanner.scan(r'(<\-)|(>=)|(<=)|%|<|>|-|\+|\*|\=|(<>)|\/|\.|:|,'):
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:449:                elif not self.is_portugol and scanner.scan(r'[-+*\/=<>:;,.@\^]'):
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:452:                    if collect_labels and scanner.match == ';':
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:454:                elif scanner.scan(r'[\(\)\[\]]+'):
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:462:                        if scanner.match == '(':
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:464:                        elif scanner.match == ')':
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:466:                        elif scanner.match == '[':
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:468:                        elif scanner.match == ']':
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:470:                elif scanner.scan(r'[A-Za-z_][A-Za-z_0-9]*'):
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:471:                    lowercase_name = scanner.match.lower()
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:523:                        if not self.is_portugol and scanner.test(r'\s*\.\s*'):
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:531:                                block_labels.add(scanner.match.lower())
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:541:                        block_labels.add(scanner.match.lower())
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:557:                elif self.is_portugol and scanner.scan(r"\""):
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:560:                elif not self.is_portugol and scanner.scan(r"'"):
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:563:                elif not self.is_portugol and scanner.scan(r'\#(\d+|\$[0-9A-Fa-f]+)'):
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:565:                elif not self.is_portugol and scanner.scan(r'\$[0-9A-Fa-f]+'):
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:567:                elif scanner.scan(r'\d+(?![eE]|\.[^.])'):
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:569:                elif scanner.scan(r'\d+(\.\d+([eE][+-]?\d+)?|[eE][+-]?\d+)'):
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:575:                    scanner.get_char()
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:579:                    if scanner.scan(r"''"):
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:581:                    elif scanner.scan(r"\""):
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:584:                    elif scanner.scan(r"[^\"]*"):
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:587:                        scanner.get_char()
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:590:                    if scanner.scan(r"''"):
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:592:                    elif scanner.scan(r"'"):
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:595:                    elif scanner.scan(r"[^']*"):
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:598:                        scanner.get_char()
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:601:                if scanner.scan(r'\s+'):
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:603:                elif scanner.scan(r'end'):
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:606:                elif scanner.scan(r'\{.*?\}|\(\*.*?\*\)'):
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:607:                    if scanner.match.startswith('$'):
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:611:                elif scanner.scan(r'//.*?$'):
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:613:                elif scanner.scan(r"'"):
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:616:                elif scanner.scan(r'@@[A-Za-z_][A-Za-z_0-9]*'):
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:618:                elif scanner.scan(r'[A-Za-z_][A-Za-z_0-9]*'):
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:619:                    lowercase_name = scanner.match.lower()
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:626:                elif scanner.scan(r'[-+*\/=<>:;,.@\^]+'):
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:628:                elif scanner.scan(r'[\(\)\[\]]+'):
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:630:                elif scanner.scan(r'\$[0-9A-Fa-f]+'):
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:632:                elif scanner.scan(r'\d+(?![eE]|\.[^.])'):
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:634:                elif scanner.scan(r'\d+(\.\d+([eE][+-]?\d+)?|[eE][+-]?\d+)'):
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:637:                    scanner.get_char()
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:641:            if not self.is_portugol and scanner.match.strip():
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:642:                was_dot = scanner.match == '.'
./.venv/lib/python3.13/site-packages/pygments/lexers/pascal.py:644:            yield scanner.start_pos, token, scanner.match or ''
Binary file ./.venv/lib/python3.13/site-packages/pygments/__pycache__/scanner.cpython-313.pyc matches
Binary file ./.venv/lib/python3.13/site-packages/gherkin/__pycache__/inout.cpython-313.pyc matches
Binary file ./.venv/lib/python3.13/site-packages/gherkin/__pycache__/token_scanner.cpython-313.pyc matches
Binary file ./.venv/lib/python3.13/site-packages/gherkin/__pycache__/parser.cpython-313.pyc matches
./.venv/lib/python3.13/site-packages/gherkin/parser.py:6:from .token_scanner import TokenScanner
./.venv/lib/python3.13/site-packages/gherkin/parser.py:47:    def __init__(self, token_scanner, token_matcher, token_queue, errors):
./.venv/lib/python3.13/site-packages/gherkin/parser.py:48:        self.token_scanner = token_scanner
./.venv/lib/python3.13/site-packages/gherkin/parser.py:59:    def parse(self, token_scanner_or_str, token_matcher=None):
./.venv/lib/python3.13/site-packages/gherkin/parser.py:61:            token_scanner = TokenScanner(token_scanner_or_str) if isinstance(token_scanner_or_str, basestring) else token_scanner_or_str
./.venv/lib/python3.13/site-packages/gherkin/parser.py:63:            token_scanner = TokenScanner(token_scanner_or_str) if isinstance(token_scanner_or_str, str) else token_scanner_or_str
./.venv/lib/python3.13/site-packages/gherkin/parser.py:69:            token_scanner,
./.venv/lib/python3.13/site-packages/gherkin/parser.py:112:            return context.token_scanner.read()
./.venv/lib/python3.13/site-packages/gherkin/token_scanner.py:10:    The scanner reads a gherkin doc (typically read from a `.feature` file) and creates a token for
./.venv/lib/python3.13/site-packages/gherkin/token_scanner.py:15:    If the scanner sees a `#` language header, it will reconfigure itself dynamically to look for
./.venv/lib/python3.13/site-packages/gherkin/inout.py:4:from .token_scanner import TokenScanner
./.venv/lib/python3.13/site-packages/gherkin/inout.py:22:            token_scanner = TokenScanner(source)
./.venv/lib/python3.13/site-packages/gherkin/inout.py:25:                gherkin_document = self.parser.parse(token_scanner)
./.venv/lib/python3.13/site-packages/gevent/tests/hosts_file.txt:1522:0.0.0.0 antivirus-scanner8.com
./.venv/lib/python3.13/site-packages/gevent/tests/hosts_file.txt:1523:0.0.0.0 antivirus-scanner.com
./.venv/lib/python3.13/site-packages/gevent/tests/hosts_file.txt:1663:0.0.0.0 newwayscanner.info
./.venv/lib/python3.13/site-packages/gevent/tests/hosts_file.txt:1686:0.0.0.0 pc-scanner16.com
./.venv/lib/python3.13/site-packages/gevent/tests/hosts_file.txt:1704:0.0.0.0 scan.antispyware-free-scanner.com
./.venv/lib/python3.13/site-packages/gevent/tests/hosts_file.txt:1705:0.0.0.0 scanner.best-click-av1.info
./.venv/lib/python3.13/site-packages/gevent/tests/hosts_file.txt:1706:0.0.0.0 scanner.best-protect.info
./.venv/lib/python3.13/site-packages/gevent/tests/hosts_file.txt:1873:0.0.0.0 xponlinescanner.com
Binary file ./.venv/lib/python3.13/site-packages/markdown/extensions/__pycache__/attr_list.cpython-313.pyc matches
./.venv/lib/python3.13/site-packages/markdown/extensions/attr_list.py:60:_scanner = re.Scanner([
./.venv/lib/python3.13/site-packages/markdown/extensions/attr_list.py:75:    attrs, remainder = _scanner.scan(attrs_string)
./.venv/lib/python3.13/site-packages/jedi-0.19.2.dist-info/RECORD:1798:jedi/third_party/typeshed/third_party/2and3/simplejson/scanner.pyi,sha256=_AUT1GJRNYopQ98J8rRXtig5nCunjXLoiCf5gN9dpXo,262
./.venv/lib/python3.13/site-packages/jedi-0.19.2.dist-info/RECORD:1869:jedi/third_party/typeshed/third_party/2and3/yaml/scanner.pyi,sha256=Ly12oPYyJVNrdODBM-F9dVO1EzK3hiiItMgay0hDQAE,3573
./.venv/lib/python3.13/site-packages/black-25.9.0.dist-info/licenses/AUTHORS.md:87:- [Jakub Kadlubiec](mailto:jakub.kadlubiec@skyscanner.net)
./.venv/lib/python3.13/site-packages/lark/parsers/xearley.py:44:            This is a custom implementation of the scanner that uses the
./.venv/lib/python3.13/site-packages/lark/parsers/xearley.py:96:            # This is the core of the Earley scanner. Create an SPPF node for each Token,
./.venv/lib/python3.13/site-packages/lark/parsers/xearley.py:146:        # processed down to terminals/empty nodes to be added to the scanner for the next
Binary file ./.venv/lib/python3.13/site-packages/lark/parsers/__pycache__/earley.cpython-313.pyc matches
Binary file ./.venv/lib/python3.13/site-packages/lark/parsers/__pycache__/xearley.cpython-313.pyc matches
./.venv/lib/python3.13/site-packages/lark/parsers/earley.py:85:        which can be added to the scan list for the next scanner cycle."""
./.venv/lib/python3.13/site-packages/lark/parsers/earley.py:191:            This is a custom implementation of the scanner that uses the
./.venv/lib/python3.13/site-packages/lark/parsers/earley.py:243:        # processed down to terminals/empty nodes to be added to the scanner for the next
./.venv/lib/python3.13/site-packages/lark/parsers/earley.py:272:        # result in a non-terminal, or the scanner if they result in a terminal.
Binary file ./.venv/lib/python3.13/site-packages/lark/__pycache__/lexer.cpython-313.pyc matches
./.venv/lib/python3.13/site-packages/lark/lexer.py:308:    def __init__(self, scanner: 'Scanner'):
./.venv/lib/python3.13/site-packages/lark/lexer.py:309:        self.scanner = scanner
./.venv/lib/python3.13/site-packages/lark/lexer.py:312:        res = self.scanner.fullmatch(t.value)
./.venv/lib/python3.13/site-packages/lark/lexer.py:585:        self._scanner: Optional[Scanner] = None
./.venv/lib/python3.13/site-packages/lark/lexer.py:587:    def _build_scanner(self) -> Scanner:
./.venv/lib/python3.13/site-packages/lark/lexer.py:601:    def scanner(self) -> Scanner:
./.venv/lib/python3.13/site-packages/lark/lexer.py:602:        if self._scanner is None:
./.venv/lib/python3.13/site-packages/lark/lexer.py:603:            self._scanner = self._build_scanner()
./.venv/lib/python3.13/site-packages/lark/lexer.py:604:        return self._scanner
./.venv/lib/python3.13/site-packages/lark/lexer.py:607:        return self.scanner.match(text, pos)
./.venv/lib/python3.13/site-packages/lark/lexer.py:614:                allowed = self.scanner.allowed_types - self.ignore_types
./.venv/lib/python3.13/site-packages/jedi/third_party/typeshed/third_party/2and3/simplejson/__init__.pyi:5:from simplejson.scanner import JSONDecodeError as JSONDecodeError
./.venv/lib/python3.13/site-packages/jedi/third_party/typeshed/third_party/2and3/yaml/loader.pyi:6:from yaml.scanner import Scanner
./.venv/lib/python3.13/site-packages/jedi/third_party/typeshed/stdlib/3/_json.pyi:26:class make_scanner:
./.venv/lib/python3.13/site-packages/jedi/third_party/typeshed/stdlib/3/_json.pyi:34:    def __init__(self, context: make_scanner) -> None: ...
./.venv/lib/python3.13/site-packages/jedi/third_party/typeshed/stdlib/2/_sre.pyi:36:    def scanner(self, s: str, start: int = ..., end: int = ...) -> SRE_Scanner: ...
Binary file ./.venv/lib/python3.13/site-packages/Cython/Utils.cpython-313-darwin.so matches
Binary file ./.venv/lib/python3.13/site-packages/Cython/__pycache__/Utils.cpython-313.pyc matches
./.venv/lib/python3.13/site-packages/Cython/Utils.py:385:    This could be added to the scanner, but it's *substantially* easier
./.venv/lib/python3.13/site-packages/Cython/Plex/Scanners.pxd:19:    cdef tuple current_scanner_position_tuple
./.venv/lib/python3.13/site-packages/Cython/Plex/__init__.py:20:   State            For defining scanner states when creating a
Binary file ./.venv/lib/python3.13/site-packages/Cython/Plex/__pycache__/Lexicons.cpython-313.pyc matches
Binary file ./.venv/lib/python3.13/site-packages/Cython/Plex/__pycache__/Errors.cpython-313.pyc matches
Binary file ./.venv/lib/python3.13/site-packages/Cython/Plex/__pycache__/Scanners.cpython-313.pyc matches
Binary file ./.venv/lib/python3.13/site-packages/Cython/Plex/__pycache__/__init__.cpython-313.pyc matches
./.venv/lib/python3.13/site-packages/Cython/Plex/Errors.py:37:    scanner = None
./.venv/lib/python3.13/site-packages/Cython/Plex/Errors.py:41:    def __init__(self, scanner, state_name):
./.venv/lib/python3.13/site-packages/Cython/Plex/Errors.py:42:        self.scanner = scanner
./.venv/lib/python3.13/site-packages/Cython/Plex/Errors.py:43:        self.position = scanner.get_position()
Binary file ./.venv/lib/python3.13/site-packages/Cython/Plex/Scanners.cpython-313-darwin.so matches
./.venv/lib/python3.13/site-packages/Cython/Plex/Lexicons.py:68:             function(scanner, text)
./.venv/lib/python3.13/site-packages/Cython/Plex/Lexicons.py:70:          where |scanner| is the relevant Scanner instance, and |text|
./.venv/lib/python3.13/site-packages/Cython/Plex/Lexicons.py:90:    At any given time, the scanner is in one of a number of states.
./.venv/lib/python3.13/site-packages/Cython/Plex/Lexicons.py:98:    The initial state of the scanner is the default state. The state can
./.venv/lib/python3.13/site-packages/Cython/Plex/Scanners.py:41:        Causes scanner to change state.
./.venv/lib/python3.13/site-packages/Cython/Plex/Scanners.py:54:    #  These positions are used by the scanner to track its internal state:
./.venv/lib/python3.13/site-packages/Cython/Plex/Scanners.py:61:    #  current_scanner_position_tuple = ("", 0, 0)
./.venv/lib/python3.13/site-packages/Cython/Plex/Scanners.py:95:        self.current_scanner_position_tuple = ("", 0, 0)
./.venv/lib/python3.13/site-packages/Cython/Plex/Scanners.py:140:        return self.current_scanner_position_tuple
./.venv/lib/python3.13/site-packages/Cython/Plex/Scanners.py:149:        self.current_scanner_position_tuple = (
./.venv/lib/python3.13/site-packages/Cython/Plex/Scanners.py:332:        """Set the current state of the scanner to the named state."""
./.venv/lib/python3.13/site-packages/Cython/Plex/Scanners.py:350:        self.queue.append(((value, text), self.current_scanner_position_tuple))
./.venv/lib/python3.13/site-packages/Cython/Compiler/Parsing.py:823:        # This is actually prevented by the scanner (Lexicon.py).
./.venv/lib/python3.13/site-packages/Cython/Compiler/Parsing.py:1351:    scanner = PyrexScanner(buf, expr_pos[0], parent_scanner=s, source_encoding=s.source_encoding, initial_pos=expr_pos)
./.venv/lib/python3.13/site-packages/Cython/Compiler/Parsing.py:1352:    expr = p_testlist(scanner)  # TODO is testlist right here?
./.venv/lib/python3.13/site-packages/Cython/Compiler/Parsing.py:3192:        # scanner returns '**' as a single token
Binary file ./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/__pycache__/TestScanning.cpython-313.pyc matches
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:21:    def make_scanner(self):
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:30:        scanner = self.make_scanner()
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:32:        self.assertEqual(scanner.sy, "IDENT")
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:33:        self.assertEqual(scanner.systring, "a0")
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:34:        scanner.next()
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:35:        self.assertEqual(scanner.sy, "IDENT")
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:36:        self.assertEqual(scanner.systring, "a1")
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:37:        a1pos = scanner.last_token_position_tuple
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:39:        a2peek = scanner.peek()  # shouldn't mess up the position
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:40:        self.assertEqual(a1pos, scanner.last_token_position_tuple)
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:41:        scanner.next()
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:42:        self.assertEqual(a2peek, (scanner.sy, scanner.systring))
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:45:        while scanner.sy != "NEWLINE":
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:46:            scanner.next()
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:52:        scanner.next()
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:53:        while scanner.sy != "NEWLINE":
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:54:            line_sy.append(scanner.sy)
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:55:            line_systring.append(scanner.systring)
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:56:            line_pos.append(scanner.last_token_position_tuple)
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:57:            scanner.next()
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:62:            scanner.put_back(sy, systring, pos)
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:65:        while scanner.sy != "NEWLINE":
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:66:            self.assertEqual(scanner.sy, line_sy[n])
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:67:            self.assertEqual(scanner.systring, line_systring[n])
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:68:            self.assertEqual(scanner.last_token_position_tuple, line_pos[n])
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:69:            scanner.next()
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:75:        scanner = self.make_scanner()
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:76:        with Scanning.tentatively_scan(scanner) as errors:
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:77:            while scanner.sy != "NEWLINE":
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:78:                scanner.next()
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:81:        scanner.next()
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:82:        self.assertEqual(scanner.systring, "b0")
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:83:        pos = scanner.last_token_position_tuple
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:84:        with Scanning.tentatively_scan(scanner) as errors:
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:85:            while scanner.sy != "NEWLINE":
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:86:                scanner.next()
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:87:                if scanner.systring == "b7":
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:88:                    scanner.error("Oh no not b7!")
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:91:        self.assertEqual(scanner.systring, "b0")  # state has been restored
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:92:        self.assertEqual(scanner.last_token_position_tuple, pos)
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:93:        scanner.next()
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:94:        self.assertEqual(scanner.systring, "b1")  # and we can keep going again
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:95:        scanner.next()
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:96:        self.assertEqual(scanner.systring, "b2")  # and we can keep going again
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:98:        with Scanning.tentatively_scan(scanner) as error:
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:99:            scanner.error("Something has gone wrong with the current symbol")
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:100:        self.assertEqual(scanner.systring, "b2")
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:101:        scanner.next()
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:102:        self.assertEqual(scanner.systring, "b3")
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:105:        sy1, systring1 = scanner.sy, scanner.systring
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:106:        pos1 = scanner.last_token_position_tuple
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:107:        with Scanning.tentatively_scan(scanner):
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:108:            scanner.next()
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:109:            sy2, systring2 = scanner.sy, scanner.systring
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:110:            pos2 = scanner.last_token_position_tuple
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:111:            with Scanning.tentatively_scan(scanner):
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:112:                with Scanning.tentatively_scan(scanner):
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:113:                    scanner.next()
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:114:                    scanner.next()
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:115:                    scanner.error("Ooops")
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:116:                self.assertEqual((scanner.sy, scanner.systring), (sy2, systring2))
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:117:            self.assertEqual((scanner.sy, scanner.systring), (sy2, systring2))
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:118:            scanner.error("eee")
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:119:        self.assertEqual((scanner.sy, scanner.systring), (sy1, systring1))
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:120:        with Scanning.tentatively_scan(scanner):
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:121:            scanner.next()
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:122:            scanner.next()
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:123:            with Scanning.tentatively_scan(scanner):
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:124:                scanner.next()
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:126:            scanner.next()
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:127:            scanner.error("Oooops")
./.venv/lib/python3.13/site-packages/Cython/Compiler/Tests/TestScanning.py:128:        self.assertEqual((scanner.sy, scanner.systring), (sy1, systring1))
./.venv/lib/python3.13/site-packages/Cython/Compiler/TreeFragment.py:78:    scanner = PyrexScanner(buf, code_source, source_encoding = encoding,
./.venv/lib/python3.13/site-packages/Cython/Compiler/TreeFragment.py:83:        tree = Parsing.p_module(scanner, 0, module_name, ctx=ctx)
./.venv/lib/python3.13/site-packages/Cython/Compiler/TreeFragment.py:87:        tree = Parsing.p_code(scanner, level=level, ctx=ctx)
Binary file ./.venv/lib/python3.13/site-packages/Cython/Compiler/__pycache__/Scanning.cpython-313.pyc matches
Binary file ./.venv/lib/python3.13/site-packages/Cython/Compiler/__pycache__/TreeFragment.cpython-313.pyc matches
Binary file ./.venv/lib/python3.13/site-packages/Cython/Compiler/__pycache__/Parsing.cpython-313.pyc matches
Binary file ./.venv/lib/python3.13/site-packages/Cython/Compiler/Parsing.cpython-313-darwin.so matches
Binary file ./.venv/lib/python3.13/site-packages/Cython/Compiler/Scanning.cpython-313-darwin.so matches
./.venv/lib/python3.13/site-packages/Cython/Compiler/Lexicon.py:153:        #debug_flags = scanner_debug_flags,
./.venv/lib/python3.13/site-packages/Cython/Compiler/Lexicon.py:154:        #debug_file = scanner_dump_file
./.venv/lib/python3.13/site-packages/Cython/Compiler/Scanning.py:24:debug_scanner = 0
./.venv/lib/python3.13/site-packages/Cython/Compiler/Scanning.py:25:trace_scanner = 0
./.venv/lib/python3.13/site-packages/Cython/Compiler/Scanning.py:26:scanner_debug_flags = 0
./.venv/lib/python3.13/site-packages/Cython/Compiler/Scanning.py:27:scanner_dump_file = None
./.venv/lib/python3.13/site-packages/Cython/Compiler/Scanning.py:299:    def __init__(self, file, filename, parent_scanner=None,
./.venv/lib/python3.13/site-packages/Cython/Compiler/Scanning.py:313:        if parent_scanner:
./.venv/lib/python3.13/site-packages/Cython/Compiler/Scanning.py:314:            self.context = parent_scanner.context
./.venv/lib/python3.13/site-packages/Cython/Compiler/Scanning.py:315:            self.included_files = parent_scanner.included_files
./.venv/lib/python3.13/site-packages/Cython/Compiler/Scanning.py:316:            self.compile_time_env = parent_scanner.compile_time_env
./.venv/lib/python3.13/site-packages/Cython/Compiler/Scanning.py:317:            self.compile_time_eval = parent_scanner.compile_time_eval
./.venv/lib/python3.13/site-packages/Cython/Compiler/Scanning.py:318:            self.compile_time_expr = parent_scanner.compile_time_expr
./.venv/lib/python3.13/site-packages/Cython/Compiler/Scanning.py:320:            if parent_scanner.async_enabled:
./.venv/lib/python3.13/site-packages/Cython/Compiler/Scanning.py:332:        self.trace = trace_scanner
./.venv/lib/python3.13/site-packages/Cython/Compiler/Scanning.py:453:        if False:  # debug_scanner:
./.venv/lib/python3.13/site-packages/Cython/Compiler/Scanning.py:547:def tentatively_scan(scanner: PyrexScanner):
./.venv/lib/python3.13/site-packages/Cython/Compiler/Scanning.py:550:        put_back_on_failure = scanner.put_back_on_failure
./.venv/lib/python3.13/site-packages/Cython/Compiler/Scanning.py:551:        scanner.put_back_on_failure = []
./.venv/lib/python3.13/site-packages/Cython/Compiler/Scanning.py:552:        initial_state = (scanner.sy, scanner.systring, scanner.position())
./.venv/lib/python3.13/site-packages/Cython/Compiler/Scanning.py:559:                if scanner.put_back_on_failure:
./.venv/lib/python3.13/site-packages/Cython/Compiler/Scanning.py:560:                    for put_back in reversed(scanner.put_back_on_failure[:-1]):
./.venv/lib/python3.13/site-packages/Cython/Compiler/Scanning.py:561:                        scanner.put_back(*put_back)
./.venv/lib/python3.13/site-packages/Cython/Compiler/Scanning.py:563:                    scanner.put_back(*initial_state)
./.venv/lib/python3.13/site-packages/Cython/Compiler/Scanning.py:567:                put_back_on_failure.extend(scanner.put_back_on_failure)
./.venv/lib/python3.13/site-packages/Cython/Compiler/Scanning.py:568:            scanner.put_back_on_failure = put_back_on_failure
./.venv/lib/python3.13/site-packages/tensorflow-2.20.0.dist-info/RECORD:8761:tensorflow/include/tensorflow/core/lib/strings/scanner.h,sha256=Pg08DEu-Nq5WuwEPH5OemXvzNsbl1R4qZuCCbrWfzN4,886
./.venv/lib/python3.13/site-packages/tensorflow-2.20.0.dist-info/RECORD:8838:tensorflow/include/tensorflow/core/platform/scanner.h,sha256=qlxt9BEaUqCGkoEezLVDX7ger15PDC73bq7RpX-nuIo,1005
./.venv/lib/python3.13/site-packages/tensorflow-2.20.0.dist-info/RECORD:9164:tensorflow/include/tensorflow/tsl/platform/scanner.h,sha256=AzH3QGU9xqyY3e_9VKJsSSVAnWbt2PoWmUX6dDX4wUY,8098
./.venv/lib/python3.13/site-packages/tensorflow-2.20.0.dist-info/RECORD:9265:tensorflow/include/tsl/platform/scanner.h,sha256=AzH3QGU9xqyY3e_9VKJsSSVAnWbt2PoWmUX6dDX4wUY,8098
./.venv/lib/python3.13/site-packages/faker/providers/lorem/it_IT/__init__.py:2201:            "scannerizzarono",
Binary file ./.venv/lib/python3.13/site-packages/faker/providers/lorem/it_IT/__pycache__/__init__.cpython-313.pyc matches
./.venv/lib/python3.13/site-packages/pip/_vendor/pygments/scanner.py:2:    pygments.scanner
./.venv/lib/python3.13/site-packages/pip/_vendor/pygments/scanner.py:5:    This library implements a regex based scanner. Some languages
./.venv/lib/python3.13/site-packages/pip/_vendor/pygments/scanner.py:12:    this scanner.
./.venv/lib/python3.13/site-packages/pip/_vendor/pygments/scanner.py:29:    Simple scanner
./.venv/lib/python3.13/site-packages/pip/_vendor/pygments/scanner.py:50:        """`True` if the scanner reached the end of text."""
Binary file ./.venv/lib/python3.13/site-packages/pip/_vendor/pygments/__pycache__/scanner.cpython-313.pyc matches
Binary file ./.venv/lib/python3.13/site-packages/h5py/.dylibs/libhdf5_hl.310.dylib matches
./.venv/lib/python3.13/site-packages/gherkin_official-29.0.0.dist-info/RECORD:19:gherkin/__pycache__/token_scanner.cpython-313.pyc,,
./.venv/lib/python3.13/site-packages/gherkin_official-29.0.0.dist-info/RECORD:47:gherkin/token_scanner.py,sha256=hlhXs4t0CXz9vxIZAwzQT2u_VqdeGArpllFCEKZRETw,1442
./.venv/lib/python3.13/site-packages/tensorflow/include/tsl/platform/scanner.h:45:    // in scanner_test.cc
./.venv/lib/python3.13/site-packages/tensorflow/include/tensorflow/core/util/proto/decode.h:362:  // own scanner but this is simpler for now.
./.venv/lib/python3.13/site-packages/tensorflow/include/tensorflow/core/util/proto/decode.h:368:  // own scanner as described above.  We would first need to obtain the length
./.venv/lib/python3.13/site-packages/tensorflow/include/tensorflow/core/framework/resource_handle.pb_text-impl.h:12:#include "tensorflow/core/lib/strings/scanner.h"
./.venv/lib/python3.13/site-packages/tensorflow/include/tensorflow/core/framework/resource_handle.pb_text-impl.h:22:    ::tensorflow::strings::Scanner* scanner, bool nested, bool close_curly,
./.venv/lib/python3.13/site-packages/tensorflow/include/tensorflow/core/framework/resource_handle.pb_text-impl.h:29:    ::tensorflow::strings::Scanner* scanner, bool nested, bool close_curly,
./.venv/lib/python3.13/site-packages/tensorflow/include/tensorflow/core/framework/attr_value.pb_text-impl.h:16:#include "tensorflow/core/lib/strings/scanner.h"
./.venv/lib/python3.13/site-packages/tensorflow/include/tensorflow/core/framework/attr_value.pb_text-impl.h:26:    ::tensorflow::strings::Scanner* scanner, bool nested, bool close_curly,
./.venv/lib/python3.13/site-packages/tensorflow/include/tensorflow/core/framework/attr_value.pb_text-impl.h:33:    ::tensorflow::strings::Scanner* scanner, bool nested, bool close_curly,
./.venv/lib/python3.13/site-packages/tensorflow/include/tensorflow/core/framework/attr_value.pb_text-impl.h:40:    ::tensorflow::strings::Scanner* scanner, bool nested, bool close_curly,
./.venv/lib/python3.13/site-packages/tensorflow/include/tensorflow/core/framework/tensor.pb_text-impl.h:14:#include "tensorflow/core/lib/strings/scanner.h"
./.venv/lib/python3.13/site-packages/tensorflow/include/tensorflow/core/framework/tensor.pb_text-impl.h:24:    ::tensorflow::strings::Scanner* scanner, bool nested, bool close_curly,
./.venv/lib/python3.13/site-packages/tensorflow/include/tensorflow/core/framework/tensor.pb_text-impl.h:31:    ::tensorflow::strings::Scanner* scanner, bool nested, bool close_curly,
./.venv/lib/python3.13/site-packages/tensorflow/include/tensorflow/core/framework/types.pb_text-impl.h:8:#include "tensorflow/core/lib/strings/scanner.h"
./.venv/lib/python3.13/site-packages/tensorflow/include/tensorflow/core/framework/types.pb_text-impl.h:18:    ::tensorflow::strings::Scanner* scanner, bool nested, bool close_curly,
./.venv/lib/python3.13/site-packages/tensorflow/include/tensorflow/core/framework/tensor_shape.pb_text-impl.h:8:#include "tensorflow/core/lib/strings/scanner.h"
./.venv/lib/python3.13/site-packages/tensorflow/include/tensorflow/core/framework/tensor_shape.pb_text-impl.h:18:    ::tensorflow::strings::Scanner* scanner, bool nested, bool close_curly,
./.venv/lib/python3.13/site-packages/tensorflow/include/tensorflow/core/framework/tensor_shape.pb_text-impl.h:25:    ::tensorflow::strings::Scanner* scanner, bool nested, bool close_curly,
./.venv/lib/python3.13/site-packages/tensorflow/include/tensorflow/core/platform/scanner.h:19:#include "tsl/platform/scanner.h"
./.venv/lib/python3.13/site-packages/tensorflow/include/tensorflow/core/lib/strings/proto_text_util.h:26:#include "tensorflow/core/platform/scanner.h"
./.venv/lib/python3.13/site-packages/tensorflow/include/tensorflow/core/lib/strings/proto_text_util.h:128:inline void ProtoSpaceAndComments(Scanner* scanner) {
./.venv/lib/python3.13/site-packages/tensorflow/include/tensorflow/core/lib/strings/proto_text_util.h:130:    scanner->AnySpace();
./.venv/lib/python3.13/site-packages/tensorflow/include/tensorflow/core/lib/strings/proto_text_util.h:131:    if (scanner->Peek() != '#') return;
./.venv/lib/python3.13/site-packages/tensorflow/include/tensorflow/core/lib/strings/proto_text_util.h:133:    while (scanner->Peek('\n') != '\n') scanner->One(Scanner::ALL);
./.venv/lib/python3.13/site-packages/tensorflow/include/tensorflow/core/lib/strings/proto_text_util.h:137:// Parse the next numeric value from <scanner>, returning false if parsing
./.venv/lib/python3.13/site-packages/tensorflow/include/tensorflow/core/lib/strings/proto_text_util.h:140:bool ProtoParseNumericFromScanner(Scanner* scanner, T* value) {
./.venv/lib/python3.13/site-packages/tensorflow/include/tensorflow/core/lib/strings/proto_text_util.h:142:  scanner->RestartCapture();
./.venv/lib/python3.13/site-packages/tensorflow/include/tensorflow/core/lib/strings/proto_text_util.h:143:  if (!scanner->Many(Scanner::LETTER_DIGIT_DOT_PLUS_MINUS)
./.venv/lib/python3.13/site-packages/tensorflow/include/tensorflow/core/lib/strings/proto_text_util.h:159:  ProtoSpaceAndComments(scanner);
./.venv/lib/python3.13/site-packages/tensorflow/include/tensorflow/core/lib/strings/proto_text_util.h:163:// Parse the next boolean value from <scanner>, returning false if parsing
./.venv/lib/python3.13/site-packages/tensorflow/include/tensorflow/core/lib/strings/proto_text_util.h:165:bool ProtoParseBoolFromScanner(Scanner* scanner, bool* value);
./.venv/lib/python3.13/site-packages/tensorflow/include/tensorflow/core/lib/strings/proto_text_util.h:167:// Parse the next string literal from <scanner>, returning false if parsing
./.venv/lib/python3.13/site-packages/tensorflow/include/tensorflow/core/lib/strings/proto_text_util.h:169:bool ProtoParseStringLiteralFromScanner(Scanner* scanner, std::string* value);
./.venv/lib/python3.13/site-packages/tensorflow/include/tensorflow/core/lib/strings/scanner.h:19:#include "tensorflow/core/platform/scanner.h"  // IWYU pragma: export
./.venv/lib/python3.13/site-packages/tensorflow/include/tensorflow/tsl/platform/scanner.h:45:    // in scanner_test.cc
./.venv/lib/python3.13/site-packages/tensorflow/include/external/icu/icu4c/source/common/rbbiscan.h:39://                        The output of the scanner is parse trees for
./.venv/lib/python3.13/site-packages/tensorflow/include/external/icu/icu4c/source/common/rbbisetb.h:70://      Starting with the rules parse tree from the scanner,
./.venv/lib/python3.13/site-packages/tensorflow/include/external/icu/icu4c/source/common/rbbirb.h:150:    RBBIRuleScanner               *fScanner;         // The scanner.
./.venv/lib/python3.13/site-packages/tensorflow/include/external/icu/icu4c/source/common/rbbirb.h:151:    RBBINode                      *fForwardTree;     // The parse trees, generated by the scanner,
./.venv/lib/python3.13/site-packages/tensorflow/include/external/icu/icu4c/source/common/rbbitblb.h:37://                         from the expression syntax tree generated by the rule scanner.
./.venv/lib/python3.13/site-packages/yaml/scanner.py:49:        """Initialize the scanner."""
./.venv/lib/python3.13/site-packages/yaml/scanner.py:338:        # In the flow context, indentation is ignored. We make the scanner less
Binary file ./.venv/lib/python3.13/site-packages/yaml/__pycache__/scanner.cpython-313.pyc matches
Binary file ./.venv/lib/python3.13/site-packages/yaml/__pycache__/loader.cpython-313.pyc matches
Binary file ./.venv/lib/python3.13/site-packages/yaml/__pycache__/parser.cpython-313.pyc matches
./.venv/lib/python3.13/site-packages/yaml/parser.py:67:from .scanner import *
./.venv/lib/python3.13/site-packages/yaml/loader.py:5:from .scanner import *
./.venv/lib/python3.13/site-packages/pygments-2.19.2.dist-info/RECORD:21:pygments/__pycache__/scanner.cpython-313.pyc,,
./.venv/lib/python3.13/site-packages/pygments-2.19.2.dist-info/RECORD:583:pygments/scanner.py,sha256=nNcETRR1tRuiTaHmHSTTECVYFPcLf6mDZu1e4u91A9E,3092
./.venv/lib/python3.13/site-packages/jupyterlab/static/7445.7c793c8e1720f8ec4f85.js:1:"use strict";(self["webpackChunk_jupyterlab_application_top"]=self["webpackChunk_jupyterlab_application_top"]||[]).push([[7445],{57445:(e,t,n)=>{n.r(t);n.d(t,{oz:()=>g});function r(e){return new RegExp("^(("+e.join(")|(")+"))\\b")}var a=/[\^@!\|<>#~\.\*\-\+\\/,=]/;var i=/(<-)|(:=)|(=<)|(>=)|(<=)|(<:)|(>:)|(=:)|(\\=)|(\\=:)|(!!)|(==)|(::)/;var u=/(:::)|(\.\.\.)|(=<:)|(>=:)/;var o=["in","then","else","of","elseof","elsecase","elseif","catch","finally","with","require","prepare","import","export","define","do"];var c=["end"];var f=r(["true","false","nil","unit"]);var s=r(["andthen","at","attr","declare","feat","from","lex","mod","div","mode","orelse","parser","prod","prop","scanner","self","syn","token"]);var l=r(["local","proc","fun","case","class","if","cond","or","dis","choice","not","thread","try","raise","lock","for","suchthat","meth","functor"]);var h=r(o);var d=r(c);function m(e,t){if(e.eatSpace()){return null}if(e.match(/[{}]/)){return"bracket"}if(e.match("[]")){return"keyword"}if(e.match(u)||e.match(i)){return"operator"}if(e.match(f)){return"atom"}var n=e.match(l);if(n){if(!t.doInCurrentLine)t.currentIndent++;else t.doInCurrentLine=false;if(n[0]=="proc"||n[0]=="fun")t.tokenize=z;else if(n[0]=="class")t.tokenize=p;else if(n[0]=="meth")t.tokenize=k;return"keyword"}if(e.match(h)||e.match(s)){return"keyword"}if(e.match(d)){t.currentIndent--;return"keyword"}var r=e.next();if(r=='"'||r=="'"){t.tokenize=b(r);return t.tokenize(e,t)}if(/[~\d]/.test(r)){if(r=="~"){if(!/^[0-9]/.test(e.peek()))return null;else if(e.next()=="0"&&e.match(/^[xX][0-9a-fA-F]+/)||e.match(/^[0-9]*(\.[0-9]+)?([eE][~+]?[0-9]+)?/))return"number"}if(r=="0"&&e.match(/^[xX][0-9a-fA-F]+/)||e.match(/^[0-9]*(\.[0-9]+)?([eE][~+]?[0-9]+)?/))return"number";return null}if(r=="%"){e.skipToEnd();return"comment"}else if(r=="/"){if(e.eat("*")){t.tokenize=v;return v(e,t)}}if(a.test(r)){return"operator"}e.eatWhile(/\w/);return"variable"}function p(e,t){if(e.eatSpace()){return null}e.match(/([A-Z][A-Za-z0-9_]*)|(`.+`)/);t.tokenize=m;return"type"}function k(e,t){if(e.eatSpace()){return null}e.match(/([a-zA-Z][A-Za-z0-9_]*)|(`.+`)/);t.tokenize=m;return"def"}function z(e,t){if(e.eatSpace()){return null}if(!t.hasPassedFirstStage&&e.eat("{")){t.hasPassedFirstStage=true;return"bracket"}else if(t.hasPassedFirstStage){e.match(/([A-Z][A-Za-z0-9_]*)|(`.+`)|\$/);t.hasPassedFirstStage=false;t.tokenize=m;return"def"}else{t.tokenize=m;return null}}function v(e,t){var n=false,r;while(r=e.next()){if(r=="/"&&n){t.tokenize=m;break}n=r=="*"}return"comment"}function b(e){return function(t,n){var r=false,a,i=false;while((a=t.next())!=null){if(a==e&&!r){i=true;break}r=!r&&a=="\\"}if(i||!r)n.tokenize=m;return"string"}}function w(){var e=o.concat(c);return new RegExp("[\\[\\]]|("+e.join("|")+")$")}const g={name:"oz",startState:function(){return{tokenize:m,currentIndent:0,doInCurrentLine:false,hasPassedFirstStage:false}},token:function(e,t){if(e.sol())t.doInCurrentLine=0;return t.tokenize(e,t)},indent:function(e,t,n){var r=t.replace(/^\s+|\s+$/g,"");if(r.match(d)||r.match(h)||r.match(/(\[])/))return n.unit*(e.currentIndent-1);if(e.currentIndent<0)return 0;return e.currentIndent*n.unit},languageData:{indentOnInut:w(),commentTokens:{line:"%",block:{open:"/*",close:"*/"}}}}}}]);
./.venv/lib/python3.13/site-packages/notebook/static/632.c59cde46a58f6dac3b70.js:25:                                 "mod", "div", "mode", "orelse", "parser", "prod", "prop", "scanner", "self", "syn", "token"]);
./.venv/lib/python3.13/site-packages/notebook/static/8479.1807152edb3d746c4d0b.js.map:1:{"version":3,"file":"8479.1807152edb3d746c4d0b.js?v=1807152edb3d746c4d0b","mappings":";;;;;;;;;;;;;;;;;;;;;;;AAAO;AACP,gDAAgD;AAChD,8CAA8C,0CAA0C;AACxF;AACA;AACA;AACA,iDAAiD;AACjD,+CAA+C;AAC/C,gDAAgD,iDAAiD;AACjG;AACA,sDAAsD;AACtD,uDAAuD;;AAEvD;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA,MAAM,yEAAyE;AAC/E;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA,MAAM;AACN;AACA;AACA,MAAM;AACN;AACA;AACA;AACA,MAAM,iBAAiB;AACvB,2DAA2D,wDAAwD;AACnH;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,cAAc;AACd,KAAK;;AAEL;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA,gBAAgB;AAChB,OAAO;AACP,sBAAsB,uBAAuB;AAC7C;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,cAAc;AACd,kBAAkB,kBAAkB;AACpC;AACA;;AAEA;;AAEA;AACO;AACP;AACA;AACA;AACA;AACA;AACA,CAAC;;AAEM;AACP;AACA;AACA;AACA;AACA;AACA,kBAAkB;AAClB,mBAAmB;AACnB;AACA;AACA;AACA;AACA;AACA,CAAC;;AAEM;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAEM;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACO;AACP,aAAa;AACb;AACA;AACA;AACA;AACA,cAAc;AACd;AACA;AACA,CAAC;;AAED;AACO;AACP;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACO;AACP;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAEM;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACO;AACP;AACA;AACA;AACA;AACA,CAAC;;AAED;AACO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACO;AACP;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACO;AACP;AACA;AACA;AACA,aAAa;AACb;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA","sources":["webpack://_JUPYTERLAB.CORE_OUTPUT/../node_modules/@codemirror/legacy-modes/mode/sql.js"],"sourcesContent":["export function sql(parserConfig) {\n  var client         = parserConfig.client || {},\n      atoms          = parserConfig.atoms || {\"false\": true, \"true\": true, \"null\": true},\n      builtin        = parserConfig.builtin || set(defaultBuiltin),\n      keywords       = parserConfig.keywords || set(sqlKeywords),\n      operatorChars  = parserConfig.operatorChars || /^[*+\\-%<>!=&|~^\\/]/,\n      support        = parserConfig.support || {},\n      hooks          = parserConfig.hooks || {},\n      dateSQL        = parserConfig.dateSQL || {\"date\" : true, \"time\" : true, \"timestamp\" : true},\n      backslashStringEscapes = parserConfig.backslashStringEscapes !== false,\n      brackets       = parserConfig.brackets || /^[\\{}\\(\\)\\[\\]]/,\n      punctuation    = parserConfig.punctuation || /^[;.,:]/\n\n  function tokenBase(stream, state) {\n    var ch = stream.next();\n\n    // call hooks from the mime type\n    if (hooks[ch]) {\n      var result = hooks[ch](stream, state);\n      if (result !== false) return result;\n    }\n\n    if (support.hexNumber &&\n      ((ch == \"0\" && stream.match(/^[xX][0-9a-fA-F]+/))\n      || (ch == \"x\" || ch == \"X\") && stream.match(/^'[0-9a-fA-F]*'/))) {\n      // hex\n      // ref: http://dev.mysql.com/doc/refman/5.5/en/hexadecimal-literals.html\n      return \"number\";\n    } else if (support.binaryNumber &&\n      (((ch == \"b\" || ch == \"B\") && stream.match(/^'[01]+'/))\n      || (ch == \"0\" && stream.match(/^b[01]*/)))) {\n      // bitstring\n      // ref: http://dev.mysql.com/doc/refman/5.5/en/bit-field-literals.html\n      return \"number\";\n    } else if (ch.charCodeAt(0) > 47 && ch.charCodeAt(0) < 58) {\n      // numbers\n      // ref: http://dev.mysql.com/doc/refman/5.5/en/number-literals.html\n      stream.match(/^[0-9]*(\\.[0-9]+)?([eE][-+]?[0-9]+)?/);\n      support.decimallessFloat && stream.match(/^\\.(?!\\.)/);\n      return \"number\";\n    } else if (ch == \"?\" && (stream.eatSpace() || stream.eol() || stream.eat(\";\"))) {\n      // placeholders\n      return \"macroName\";\n    } else if (ch == \"'\" || (ch == '\"' && support.doubleQuote)) {\n      // strings\n      // ref: http://dev.mysql.com/doc/refman/5.5/en/string-literals.html\n      state.tokenize = tokenLiteral(ch);\n      return state.tokenize(stream, state);\n    } else if ((((support.nCharCast && (ch == \"n\" || ch == \"N\"))\n        || (support.charsetCast && ch == \"_\" && stream.match(/[a-z][a-z0-9]*/i)))\n        && (stream.peek() == \"'\" || stream.peek() == '\"'))) {\n      // charset casting: _utf8'str', N'str', n'str'\n      // ref: http://dev.mysql.com/doc/refman/5.5/en/string-literals.html\n      return \"keyword\";\n    } else if (support.escapeConstant && (ch == \"e\" || ch == \"E\")\n        && (stream.peek() == \"'\" || (stream.peek() == '\"' && support.doubleQuote))) {\n      // escape constant: E'str', e'str'\n      // ref: https://www.postgresql.org/docs/current/sql-syntax-lexical.html#SQL-SYNTAX-STRINGS-ESCAPE\n      state.tokenize = function(stream, state) {\n        return (state.tokenize = tokenLiteral(stream.next(), true))(stream, state);\n      }\n      return \"keyword\";\n    } else if (support.commentSlashSlash && ch == \"/\" && stream.eat(\"/\")) {\n      // 1-line comment\n      stream.skipToEnd();\n      return \"comment\";\n    } else if ((support.commentHash && ch == \"#\")\n        || (ch == \"-\" && stream.eat(\"-\") && (!support.commentSpaceRequired || stream.eat(\" \")))) {\n      // 1-line comments\n      // ref: https://kb.askmonty.org/en/comment-syntax/\n      stream.skipToEnd();\n      return \"comment\";\n    } else if (ch == \"/\" && stream.eat(\"*\")) {\n      // multi-line comments\n      // ref: https://kb.askmonty.org/en/comment-syntax/\n      state.tokenize = tokenComment(1);\n      return state.tokenize(stream, state);\n    } else if (ch == \".\") {\n      // .1 for 0.1\n      if (support.zerolessFloat && stream.match(/^(?:\\d+(?:e[+-]?\\d+)?)/i))\n        return \"number\";\n      if (stream.match(/^\\.+/))\n        return null\n      // .table_name (ODBC)\n      // // ref: http://dev.mysql.com/doc/refman/5.6/en/identifier-qualifiers.html\n      if (support.ODBCdotTable && stream.match(/^[\\w\\d_$#]+/))\n        return \"type\";\n    } else if (operatorChars.test(ch)) {\n      // operators\n      stream.eatWhile(operatorChars);\n      return \"operator\";\n    } else if (brackets.test(ch)) {\n      // brackets\n      return \"bracket\";\n    } else if (punctuation.test(ch)) {\n      // punctuation\n      stream.eatWhile(punctuation);\n      return \"punctuation\";\n    } else if (ch == '{' &&\n        (stream.match(/^( )*(d|D|t|T|ts|TS)( )*'[^']*'( )*}/) || stream.match(/^( )*(d|D|t|T|ts|TS)( )*\"[^\"]*\"( )*}/))) {\n      // dates (weird ODBC syntax)\n      // ref: http://dev.mysql.com/doc/refman/5.5/en/date-and-time-literals.html\n      return \"number\";\n    } else {\n      stream.eatWhile(/^[_\\w\\d]/);\n      var word = stream.current().toLowerCase();\n      // dates (standard SQL syntax)\n      // ref: http://dev.mysql.com/doc/refman/5.5/en/date-and-time-literals.html\n      if (dateSQL.hasOwnProperty(word) && (stream.match(/^( )+'[^']*'/) || stream.match(/^( )+\"[^\"]*\"/)))\n        return \"number\";\n      if (atoms.hasOwnProperty(word)) return \"atom\";\n      if (builtin.hasOwnProperty(word)) return \"type\";\n      if (keywords.hasOwnProperty(word)) return \"keyword\";\n      if (client.hasOwnProperty(word)) return \"builtin\";\n      return null;\n    }\n  }\n\n  // 'string', with char specified in quote escaped by '\\'\n  function tokenLiteral(quote, backslashEscapes) {\n    return function(stream, state) {\n      var escaped = false, ch;\n      while ((ch = stream.next()) != null) {\n        if (ch == quote && !escaped) {\n          state.tokenize = tokenBase;\n          break;\n        }\n        escaped = (backslashStringEscapes || backslashEscapes) && !escaped && ch == \"\\\\\";\n      }\n      return \"string\";\n    };\n  }\n  function tokenComment(depth) {\n    return function(stream, state) {\n      var m = stream.match(/^.*?(\\/\\*|\\*\\/)/)\n      if (!m) stream.skipToEnd()\n      else if (m[1] == \"/*\") state.tokenize = tokenComment(depth + 1)\n      else if (depth > 1) state.tokenize = tokenComment(depth - 1)\n      else state.tokenize = tokenBase\n      return \"comment\"\n    }\n  }\n\n  function pushContext(stream, state, type) {\n    state.context = {\n      prev: state.context,\n      indent: stream.indentation(),\n      col: stream.column(),\n      type: type\n    };\n  }\n\n  function popContext(state) {\n    state.indent = state.context.indent;\n    state.context = state.context.prev;\n  }\n\n  return {\n    name: \"sql\",\n\n    startState: function() {\n      return {tokenize: tokenBase, context: null};\n    },\n\n    token: function(stream, state) {\n      if (stream.sol()) {\n        if (state.context && state.context.align == null)\n          state.context.align = false;\n      }\n      if (state.tokenize == tokenBase && stream.eatSpace()) return null;\n\n      var style = state.tokenize(stream, state);\n      if (style == \"comment\") return style;\n\n      if (state.context && state.context.align == null)\n        state.context.align = true;\n\n      var tok = stream.current();\n      if (tok == \"(\")\n        pushContext(stream, state, \")\");\n      else if (tok == \"[\")\n        pushContext(stream, state, \"]\");\n      else if (state.context && state.context.type == tok)\n        popContext(state);\n      return style;\n    },\n\n    indent: function(state, textAfter, iCx) {\n      var cx = state.context;\n      if (!cx) return null;\n      var closing = textAfter.charAt(0) == cx.type;\n      if (cx.align) return cx.col + (closing ? 0 : 1);\n      else return cx.indent + (closing ? 0 : iCx.unit);\n    },\n\n    languageData: {\n      commentTokens: {\n        line: support.commentSlashSlash ? \"//\" : support.commentHash ? \"#\" : \"--\",\n        block: {open: \"/*\", close: \"*/\"}\n      },\n      closeBrackets: {brackets: [\"(\", \"[\", \"{\", \"'\", '\"', \"`\"]}\n    }\n  };\n};\n\n// `identifier`\nfunction hookIdentifier(stream) {\n  // MySQL/MariaDB identifiers\n  // ref: http://dev.mysql.com/doc/refman/5.6/en/identifier-qualifiers.html\n  var ch;\n  while ((ch = stream.next()) != null) {\n    if (ch == \"`\" && !stream.eat(\"`\")) return \"string.special\";\n  }\n  stream.backUp(stream.current().length - 1);\n  return stream.eatWhile(/\\w/) ? \"string.special\" : null;\n}\n\n// \"identifier\"\nfunction hookIdentifierDoublequote(stream) {\n  // Standard SQL /SQLite identifiers\n  // ref: http://web.archive.org/web/20160813185132/http://savage.net.au/SQL/sql-99.bnf.html#delimited%20identifier\n  // ref: http://sqlite.org/lang_keywords.html\n  var ch;\n  while ((ch = stream.next()) != null) {\n    if (ch == \"\\\"\" && !stream.eat(\"\\\"\")) return \"string.special\";\n  }\n  stream.backUp(stream.current().length - 1);\n  return stream.eatWhile(/\\w/) ? \"string.special\" : null;\n}\n\n// variable token\nfunction hookVar(stream) {\n  // variables\n  // @@prefix.varName @varName\n  // varName can be quoted with ` or ' or \"\n  // ref: http://dev.mysql.com/doc/refman/5.5/en/user-variables.html\n  if (stream.eat(\"@\")) {\n    stream.match('session.');\n    stream.match('local.');\n    stream.match('global.');\n  }\n\n  if (stream.eat(\"'\")) {\n    stream.match(/^.*'/);\n    return \"string.special\";\n  } else if (stream.eat('\"')) {\n    stream.match(/^.*\"/);\n    return \"string.special\";\n  } else if (stream.eat(\"`\")) {\n    stream.match(/^.*`/);\n    return \"string.special\";\n  } else if (stream.match(/^[0-9a-zA-Z$\\.\\_]+/)) {\n    return \"string.special\";\n  }\n  return null;\n};\n\n// short client keyword token\nfunction hookClient(stream) {\n  // \\N means NULL\n  // ref: http://dev.mysql.com/doc/refman/5.5/en/null-values.html\n  if (stream.eat(\"N\")) {\n    return \"atom\";\n  }\n  // \\g, etc\n  // ref: http://dev.mysql.com/doc/refman/5.5/en/mysql-commands.html\n  return stream.match(/^[a-zA-Z.#!?]/) ? \"string.special\" : null;\n}\n\n// these keywords are used by all SQL dialects (however, a mode can still overwrite it)\nvar sqlKeywords = \"alter and as asc between by count create delete desc distinct drop from group having in insert into is join like not on or order select set table union update values where limit \";\n\n// turn a space-separated list into an array\nfunction set(str) {\n  var obj = {}, words = str.split(\" \");\n  for (var i = 0; i < words.length; ++i) obj[words[i]] = true;\n  return obj;\n}\n\nvar defaultBuiltin = \"bool boolean bit blob enum long longblob longtext medium mediumblob mediumint mediumtext time timestamp tinyblob tinyint tinytext text bigint int int1 int2 int3 int4 int8 integer float float4 float8 double char varbinary varchar varcharacter precision real date datetime year unsigned signed decimal numeric\"\n\n// A generic SQL Mode. It's not a standard, it just try to support what is generally supported\nexport const standardSQL = sql({\n  keywords: set(sqlKeywords + \"begin\"),\n  builtin: set(defaultBuiltin),\n  atoms: set(\"false true null unknown\"),\n  dateSQL: set(\"date time timestamp\"),\n  support: set(\"ODBCdotTable doubleQuote binaryNumber hexNumber\")\n});\n\nexport const msSQL = sql({\n  client: set(\"$partition binary_checksum checksum connectionproperty context_info current_request_id error_line error_message error_number error_procedure error_severity error_state formatmessage get_filestream_transaction_context getansinull host_id host_name isnull isnumeric min_active_rowversion newid newsequentialid rowcount_big xact_state object_id\"),\n  keywords: set(sqlKeywords + \"begin trigger proc view index for add constraint key primary foreign collate clustered nonclustered declare exec go if use index holdlock nolock nowait paglock readcommitted readcommittedlock readpast readuncommitted repeatableread rowlock serializable snapshot tablock tablockx updlock with\"),\n  builtin: set(\"bigint numeric bit smallint decimal smallmoney int tinyint money float real char varchar text nchar nvarchar ntext binary varbinary image cursor timestamp hierarchyid uniqueidentifier sql_variant xml table \"),\n  atoms: set(\"is not null like and or in left right between inner outer join all any some cross unpivot pivot exists\"),\n  operatorChars: /^[*+\\-%<>!=^\\&|\\/]/,\n  brackets: /^[\\{}\\(\\)]/,\n  punctuation: /^[;.,:/]/,\n  backslashStringEscapes: false,\n  dateSQL: set(\"date datetimeoffset datetime2 smalldatetime datetime time\"),\n  hooks: {\n    \"@\":   hookVar\n  }\n});\n\nexport const mySQL = sql({\n  client: set(\"charset clear connect edit ego exit go help nopager notee nowarning pager print prompt quit rehash source status system tee\"),\n  keywords: set(sqlKeywords + \"accessible action add after algorithm all analyze asensitive at authors auto_increment autocommit avg avg_row_length before binary binlog both btree cache call cascade cascaded case catalog_name chain change changed character check checkpoint checksum class_origin client_statistics close coalesce code collate collation collations column columns comment commit committed completion concurrent condition connection consistent constraint contains continue contributors convert cross current current_date current_time current_timestamp current_user cursor data database databases day_hour day_microsecond day_minute day_second deallocate dec declare default delay_key_write delayed delimiter des_key_file describe deterministic dev_pop dev_samp deviance diagnostics directory disable discard distinctrow div dual dumpfile each elseif enable enclosed end ends engine engines enum errors escape escaped even event events every execute exists exit explain extended fast fetch field fields first flush for force foreign found_rows full fulltext function general get global grant grants group group_concat handler hash help high_priority hosts hour_microsecond hour_minute hour_second if ignore ignore_server_ids import index index_statistics infile inner innodb inout insensitive insert_method install interval invoker isolation iterate key keys kill language last leading leave left level limit linear lines list load local localtime localtimestamp lock logs low_priority master master_heartbeat_period master_ssl_verify_server_cert masters match max max_rows maxvalue message_text middleint migrate min min_rows minute_microsecond minute_second mod mode modifies modify mutex mysql_errno natural next no no_write_to_binlog offline offset one online open optimize option optionally out outer outfile pack_keys parser partition partitions password phase plugin plugins prepare preserve prev primary privileges procedure processlist profile profiles purge query quick range read read_write reads real rebuild recover references regexp relaylog release remove rename reorganize repair repeatable replace require resignal restrict resume return returns revoke right rlike rollback rollup row row_format rtree savepoint schedule schema schema_name schemas second_microsecond security sensitive separator serializable server session share show signal slave slow smallint snapshot soname spatial specific sql sql_big_result sql_buffer_result sql_cache sql_calc_found_rows sql_no_cache sql_small_result sqlexception sqlstate sqlwarning ssl start starting starts status std stddev stddev_pop stddev_samp storage straight_join subclass_origin sum suspend table_name table_statistics tables tablespace temporary terminated to trailing transaction trigger triggers truncate uncommitted undo uninstall unique unlock upgrade usage use use_frm user user_resources user_statistics using utc_date utc_time utc_timestamp value variables varying view views warnings when while with work write xa xor year_month zerofill begin do then else loop repeat\"),\n  builtin: set(\"bool boolean bit blob decimal double float long longblob longtext medium mediumblob mediumint mediumtext time timestamp tinyblob tinyint tinytext text bigint int int1 int2 int3 int4 int8 integer float float4 float8 double char varbinary varchar varcharacter precision date datetime year unsigned signed numeric\"),\n  atoms: set(\"false true null unknown\"),\n  operatorChars: /^[*+\\-%<>!=&|^]/,\n  dateSQL: set(\"date time timestamp\"),\n  support: set(\"ODBCdotTable decimallessFloat zerolessFloat binaryNumber hexNumber doubleQuote nCharCast charsetCast commentHash commentSpaceRequired\"),\n  hooks: {\n    \"@\":   hookVar,\n    \"`\":   hookIdentifier,\n    \"\\\\\":  hookClient\n  }\n});\n\nexport const mariaDB = sql({\n  client: set(\"charset clear connect edit ego exit go help nopager notee nowarning pager print prompt quit rehash source status system tee\"),\n  keywords: set(sqlKeywords + \"accessible action add after algorithm all always analyze asensitive at authors auto_increment autocommit avg avg_row_length before binary binlog both btree cache call cascade cascaded case catalog_name chain change changed character check checkpoint checksum class_origin client_statistics close coalesce code collate collation collations column columns comment commit committed completion concurrent condition connection consistent constraint contains continue contributors convert cross current current_date current_time current_timestamp current_user cursor data database databases day_hour day_microsecond day_minute day_second deallocate dec declare default delay_key_write delayed delimiter des_key_file describe deterministic dev_pop dev_samp deviance diagnostics directory disable discard distinctrow div dual dumpfile each elseif enable enclosed end ends engine engines enum errors escape escaped even event events every execute exists exit explain extended fast fetch field fields first flush for force foreign found_rows full fulltext function general generated get global grant grants group group_concat handler hard hash help high_priority hosts hour_microsecond hour_minute hour_second if ignore ignore_server_ids import index index_statistics infile inner innodb inout insensitive insert_method install interval invoker isolation iterate key keys kill language last leading leave left level limit linear lines list load local localtime localtimestamp lock logs low_priority master master_heartbeat_period master_ssl_verify_server_cert masters match max max_rows maxvalue message_text middleint migrate min min_rows minute_microsecond minute_second mod mode modifies modify mutex mysql_errno natural next no no_write_to_binlog offline offset one online open optimize option optionally out outer outfile pack_keys parser partition partitions password persistent phase plugin plugins prepare preserve prev primary privileges procedure processlist profile profiles purge query quick range read read_write reads real rebuild recover references regexp relaylog release remove rename reorganize repair repeatable replace require resignal restrict resume return returns revoke right rlike rollback rollup row row_format rtree savepoint schedule schema schema_name schemas second_microsecond security sensitive separator serializable server session share show shutdown signal slave slow smallint snapshot soft soname spatial specific sql sql_big_result sql_buffer_result sql_cache sql_calc_found_rows sql_no_cache sql_small_result sqlexception sqlstate sqlwarning ssl start starting starts status std stddev stddev_pop stddev_samp storage straight_join subclass_origin sum suspend table_name table_statistics tables tablespace temporary terminated to trailing transaction trigger triggers truncate uncommitted undo uninstall unique unlock upgrade usage use use_frm user user_resources user_statistics using utc_date utc_time utc_timestamp value variables varying view views virtual warnings when while with work write xa xor year_month zerofill begin do then else loop repeat\"),\n  builtin: set(\"bool boolean bit blob decimal double float long longblob longtext medium mediumblob mediumint mediumtext time timestamp tinyblob tinyint tinytext text bigint int int1 int2 int3 int4 int8 integer float float4 float8 double char varbinary varchar varcharacter precision date datetime year unsigned signed numeric\"),\n  atoms: set(\"false true null unknown\"),\n  operatorChars: /^[*+\\-%<>!=&|^]/,\n  dateSQL: set(\"date time timestamp\"),\n  support: set(\"ODBCdotTable decimallessFloat zerolessFloat binaryNumber hexNumber doubleQuote nCharCast charsetCast commentHash commentSpaceRequired\"),\n  hooks: {\n    \"@\":   hookVar,\n    \"`\":   hookIdentifier,\n    \"\\\\\":  hookClient\n  }\n});\n\n// provided by the phpLiteAdmin project - phpliteadmin.org\nexport const sqlite = sql({\n  // commands of the official SQLite client, ref: https://www.sqlite.org/cli.html#dotcmd\n  client: set(\"auth backup bail binary changes check clone databases dbinfo dump echo eqp exit explain fullschema headers help import imposter indexes iotrace limit lint load log mode nullvalue once open output print prompt quit read restore save scanstats schema separator session shell show stats system tables testcase timeout timer trace vfsinfo vfslist vfsname width\"),\n  // ref: http://sqlite.org/lang_keywords.html\n  keywords: set(sqlKeywords + \"abort action add after all analyze attach autoincrement before begin cascade case cast check collate column commit conflict constraint cross current_date current_time current_timestamp database default deferrable deferred detach each else end escape except exclusive exists explain fail for foreign full glob if ignore immediate index indexed initially inner instead intersect isnull key left limit match natural no notnull null of offset outer plan pragma primary query raise recursive references regexp reindex release rename replace restrict right rollback row savepoint temp temporary then to transaction trigger unique using vacuum view virtual when with without\"),\n  // SQLite is weakly typed, ref: http://sqlite.org/datatype3.html. This is just a list of some common types.\n  builtin: set(\"bool boolean bit blob decimal double float long longblob longtext medium mediumblob mediumint mediumtext time timestamp tinyblob tinyint tinytext text clob bigint int int2 int8 integer float double char varchar date datetime year unsigned signed numeric real\"),\n  // ref: http://sqlite.org/syntax/literal-value.html\n  atoms: set(\"null current_date current_time current_timestamp\"),\n  // ref: http://sqlite.org/lang_expr.html#binaryops\n  operatorChars: /^[*+\\-%<>!=&|/~]/,\n  // SQLite is weakly typed, ref: http://sqlite.org/datatype3.html. This is just a list of some common types.\n  dateSQL: set(\"date time timestamp datetime\"),\n  support: set(\"decimallessFloat zerolessFloat\"),\n  identifierQuote: \"\\\"\",  //ref: http://sqlite.org/lang_keywords.html\n  hooks: {\n    // bind-parameters ref:http://sqlite.org/lang_expr.html#varparam\n    \"@\":   hookVar,\n    \":\":   hookVar,\n    \"?\":   hookVar,\n    \"$\":   hookVar,\n    // The preferred way to escape Identifiers is using double quotes, ref: http://sqlite.org/lang_keywords.html\n    \"\\\"\":   hookIdentifierDoublequote,\n    // there is also support for backticks, ref: http://sqlite.org/lang_keywords.html\n    \"`\":   hookIdentifier\n  }\n});\n\n// the query language used by Apache Cassandra is called CQL, but this mime type\n// is called Cassandra to avoid confusion with Contextual Query Language\nexport const cassandra = sql({\n  client: { },\n  keywords: set(\"add all allow alter and any apply as asc authorize batch begin by clustering columnfamily compact consistency count create custom delete desc distinct drop each_quorum exists filtering from grant if in index insert into key keyspace keyspaces level limit local_one local_quorum modify nan norecursive nosuperuser not of on one order password permission permissions primary quorum rename revoke schema select set storage superuser table three to token truncate ttl two type unlogged update use user users using values where with writetime\"),\n  builtin: set(\"ascii bigint blob boolean counter decimal double float frozen inet int list map static text timestamp timeuuid tuple uuid varchar varint\"),\n  atoms: set(\"false true infinity NaN\"),\n  operatorChars: /^[<>=]/,\n  dateSQL: { },\n  support: set(\"commentSlashSlash decimallessFloat\"),\n  hooks: { }\n});\n\n// this is based on Peter Raganitsch's 'plsql' mode\nexport const plSQL = sql({\n  client:     set(\"appinfo arraysize autocommit autoprint autorecovery autotrace blockterminator break btitle cmdsep colsep compatibility compute concat copycommit copytypecheck define describe echo editfile embedded escape exec execute feedback flagger flush heading headsep instance linesize lno loboffset logsource long longchunksize markup native newpage numformat numwidth pagesize pause pno recsep recsepchar release repfooter repheader serveroutput shiftinout show showmode size spool sqlblanklines sqlcase sqlcode sqlcontinue sqlnumber sqlpluscompatibility sqlprefix sqlprompt sqlterminator suffix tab term termout time timing trimout trimspool ttitle underline verify version wrap\"),\n  keywords:   set(\"abort accept access add all alter and any array arraylen as asc assert assign at attributes audit authorization avg base_table begin between binary_integer body boolean by case cast char char_base check close cluster clusters colauth column comment commit compress connect connected constant constraint crash create current currval cursor data_base database date dba deallocate debugoff debugon decimal declare default definition delay delete desc digits dispose distinct do drop else elseif elsif enable end entry escape exception exception_init exchange exclusive exists exit external fast fetch file for force form from function generic goto grant group having identified if immediate in increment index indexes indicator initial initrans insert interface intersect into is key level library like limited local lock log logging long loop master maxextents maxtrans member minextents minus mislabel mode modify multiset new next no noaudit nocompress nologging noparallel not nowait number_base object of off offline on online only open option or order out package parallel partition pctfree pctincrease pctused pls_integer positive positiven pragma primary prior private privileges procedure public raise range raw read rebuild record ref references refresh release rename replace resource restrict return returning returns reverse revoke rollback row rowid rowlabel rownum rows run savepoint schema segment select separate session set share snapshot some space split sql start statement storage subtype successful synonym tabauth table tables tablespace task terminate then to trigger truncate type union unique unlimited unrecoverable unusable update use using validate value values variable view views when whenever where while with work\"),\n  builtin:    set(\"abs acos add_months ascii asin atan atan2 average bfile bfilename bigserial bit blob ceil character chartorowid chr clob concat convert cos cosh count dec decode deref dual dump dup_val_on_index empty error exp false float floor found glb greatest hextoraw initcap instr instrb int integer isopen last_day least length lengthb ln lower lpad ltrim lub make_ref max min mlslabel mod months_between natural naturaln nchar nclob new_time next_day nextval nls_charset_decl_len nls_charset_id nls_charset_name nls_initcap nls_lower nls_sort nls_upper nlssort no_data_found notfound null number numeric nvarchar2 nvl others power rawtohex real reftohex round rowcount rowidtochar rowtype rpad rtrim serial sign signtype sin sinh smallint soundex sqlcode sqlerrm sqrt stddev string substr substrb sum sysdate tan tanh to_char text to_date to_label to_multi_byte to_number to_single_byte translate true trunc uid unlogged upper user userenv varchar varchar2 variance varying vsize xml\"),\n  operatorChars: /^[*\\/+\\-%<>!=~]/,\n  dateSQL:    set(\"date time timestamp\"),\n  support:    set(\"doubleQuote nCharCast zerolessFloat binaryNumber hexNumber\")\n});\n\n// Created to support specific hive keywords\nexport const hive = sql({\n  keywords: set(\"select alter $elem$ $key$ $value$ add after all analyze and archive as asc before between binary both bucket buckets by cascade case cast change cluster clustered clusterstatus collection column columns comment compute concatenate continue create cross cursor data database databases dbproperties deferred delete delimited desc describe directory disable distinct distribute drop else enable end escaped exclusive exists explain export extended external fetch fields fileformat first format formatted from full function functions grant group having hold_ddltime idxproperties if import in index indexes inpath inputdriver inputformat insert intersect into is items join keys lateral left like limit lines load local location lock locks mapjoin materialized minus msck no_drop nocompress not of offline on option or order out outer outputdriver outputformat overwrite partition partitioned partitions percent plus preserve procedure purge range rcfile read readonly reads rebuild recordreader recordwriter recover reduce regexp rename repair replace restrict revoke right rlike row schema schemas semi sequencefile serde serdeproperties set shared show show_database sort sorted ssl statistics stored streamtable table tables tablesample tblproperties temporary terminated textfile then tmp to touch transform trigger unarchive undo union uniquejoin unlock update use using utc utc_tmestamp view when where while with admin authorization char compact compactions conf cube current current_date current_timestamp day decimal defined dependency directories elem_type exchange file following for grouping hour ignore inner interval jar less logical macro minute month more none noscan over owner partialscan preceding pretty principals protection reload rewrite role roles rollup rows second server sets skewed transactions truncate unbounded unset uri user values window year\"),\n  builtin: set(\"bool boolean long timestamp tinyint smallint bigint int float double date datetime unsigned string array struct map uniontype key_type utctimestamp value_type varchar\"),\n  atoms: set(\"false true null unknown\"),\n  operatorChars: /^[*+\\-%<>!=]/,\n  dateSQL: set(\"date timestamp\"),\n  support: set(\"ODBCdotTable doubleQuote binaryNumber hexNumber\")\n});\n\nexport const pgSQL = sql({\n  client: set(\"source\"),\n  // For PostgreSQL - https://www.postgresql.org/docs/11/sql-keywords-appendix.html\n  // For pl/pgsql lang - https://github.com/postgres/postgres/blob/REL_11_2/src/pl/plpgsql/src/pl_scanner.c\n  keywords: set(sqlKeywords + \"a abort abs absent absolute access according action ada add admin after aggregate alias all allocate also alter always analyse analyze and any are array array_agg array_max_cardinality as asc asensitive assert assertion assignment asymmetric at atomic attach attribute attributes authorization avg backward base64 before begin begin_frame begin_partition bernoulli between bigint binary bit bit_length blob blocked bom boolean both breadth by c cache call called cardinality cascade cascaded case cast catalog catalog_name ceil ceiling chain char char_length character character_length character_set_catalog character_set_name character_set_schema characteristics characters check checkpoint class class_origin clob close cluster coalesce cobol collate collation collation_catalog collation_name collation_schema collect column column_name columns command_function command_function_code comment comments commit committed concurrently condition condition_number configuration conflict connect connection connection_name constant constraint constraint_catalog constraint_name constraint_schema constraints constructor contains content continue control conversion convert copy corr corresponding cost count covar_pop covar_samp create cross csv cube cume_dist current current_catalog current_date current_default_transform_group current_path current_role current_row current_schema current_time current_timestamp current_transform_group_for_type current_user cursor cursor_name cycle data database datalink datatype date datetime_interval_code datetime_interval_precision day db deallocate debug dec decimal declare default defaults deferrable deferred defined definer degree delete delimiter delimiters dense_rank depends depth deref derived desc describe descriptor detach detail deterministic diagnostics dictionary disable discard disconnect dispatch distinct dlnewcopy dlpreviouscopy dlurlcomplete dlurlcompleteonly dlurlcompletewrite dlurlpath dlurlpathonly dlurlpathwrite dlurlscheme dlurlserver dlvalue do document domain double drop dump dynamic dynamic_function dynamic_function_code each element else elseif elsif empty enable encoding encrypted end end_frame end_partition endexec enforced enum equals errcode error escape event every except exception exclude excluding exclusive exec execute exists exit exp explain expression extension external extract false family fetch file filter final first first_value flag float floor following for force foreach foreign fortran forward found frame_row free freeze from fs full function functions fusion g general generated get global go goto grant granted greatest group grouping groups handler having header hex hierarchy hint hold hour id identity if ignore ilike immediate immediately immutable implementation implicit import in include including increment indent index indexes indicator info inherit inherits initially inline inner inout input insensitive insert instance instantiable instead int integer integrity intersect intersection interval into invoker is isnull isolation join k key key_member key_type label lag language large last last_value lateral lead leading leakproof least left length level library like like_regex limit link listen ln load local localtime localtimestamp location locator lock locked log logged loop lower m map mapping match matched materialized max max_cardinality maxvalue member merge message message_length message_octet_length message_text method min minute minvalue mod mode modifies module month more move multiset mumps name names namespace national natural nchar nclob nesting new next nfc nfd nfkc nfkd nil no none normalize normalized not nothing notice notify notnull nowait nth_value ntile null nullable nullif nulls number numeric object occurrences_regex octet_length octets of off offset oids old on only open operator option options or order ordering ordinality others out outer output over overlaps overlay overriding owned owner p pad parallel parameter parameter_mode parameter_name parameter_ordinal_position parameter_specific_catalog parameter_specific_name parameter_specific_schema parser partial partition pascal passing passthrough password path percent percent_rank percentile_cont percentile_disc perform period permission pg_context pg_datatype_name pg_exception_context pg_exception_detail pg_exception_hint placing plans pli policy portion position position_regex power precedes preceding precision prepare prepared preserve primary print_strict_params prior privileges procedural procedure procedures program public publication query quote raise range rank read reads real reassign recheck recovery recursive ref references referencing refresh regr_avgx regr_avgy regr_count regr_intercept regr_r2 regr_slope regr_sxx regr_sxy regr_syy reindex relative release rename repeatable replace replica requiring reset respect restart restore restrict result result_oid return returned_cardinality returned_length returned_octet_length returned_sqlstate returning returns reverse revoke right role rollback rollup routine routine_catalog routine_name routine_schema routines row row_count row_number rows rowtype rule savepoint scale schema schema_name schemas scope scope_catalog scope_name scope_schema scroll search second section security select selective self sensitive sequence sequences serializable server server_name session session_user set setof sets share show similar simple size skip slice smallint snapshot some source space specific specific_name specifictype sql sqlcode sqlerror sqlexception sqlstate sqlwarning sqrt stable stacked standalone start state statement static statistics stddev_pop stddev_samp stdin stdout storage strict strip structure style subclass_origin submultiset subscription substring substring_regex succeeds sum symmetric sysid system system_time system_user t table table_name tables tablesample tablespace temp template temporary text then ties time timestamp timezone_hour timezone_minute to token top_level_count trailing transaction transaction_active transactions_committed transactions_rolled_back transform transforms translate translate_regex translation treat trigger trigger_catalog trigger_name trigger_schema trim trim_array true truncate trusted type types uescape unbounded uncommitted under unencrypted union unique unknown unlink unlisten unlogged unnamed unnest until untyped update upper uri usage use_column use_variable user user_defined_type_catalog user_defined_type_code user_defined_type_name user_defined_type_schema using vacuum valid validate validator value value_of values var_pop var_samp varbinary varchar variable_conflict variadic varying verbose version versioning view views volatile warning when whenever where while whitespace width_bucket window with within without work wrapper write xml xmlagg xmlattributes xmlbinary xmlcast xmlcomment xmlconcat xmldeclaration xmldocument xmlelement xmlexists xmlforest xmliterate xmlnamespaces xmlparse xmlpi xmlquery xmlroot xmlschema xmlserialize xmltable xmltext xmlvalidate year yes zone\"),\n  // https://www.postgresql.org/docs/11/datatype.html\n  builtin: set(\"bigint int8 bigserial serial8 bit varying varbit boolean bool box bytea character char varchar cidr circle date double precision float8 inet integer int int4 interval json jsonb line lseg macaddr macaddr8 money numeric decimal path pg_lsn point polygon real float4 smallint int2 smallserial serial2 serial serial4 text time without zone with timetz timestamp timestamptz tsquery tsvector txid_snapshot uuid xml\"),\n  atoms: set(\"false true null unknown\"),\n  operatorChars: /^[*\\/+\\-%<>!=&|^\\/#@?~]/,\n  backslashStringEscapes: false,\n  dateSQL: set(\"date time timestamp\"),\n  support: set(\"ODBCdotTable decimallessFloat zerolessFloat binaryNumber hexNumber nCharCast charsetCast escapeConstant\")\n});\n\n// Google's SQL-like query language, GQL\nexport const gql = sql({\n  keywords: set(\"ancestor and asc by contains desc descendant distinct from group has in is limit offset on order select superset where\"),\n  atoms: set(\"false true\"),\n  builtin: set(\"blob datetime first key __key__ string integer double boolean null\"),\n  operatorChars: /^[*+\\-%<>!=]/\n});\n\n// Greenplum\nexport const gpSQL = sql({\n  client: set(\"source\"),\n  //https://github.com/greenplum-db/gpdb/blob/master/src/include/parser/kwlist.h\n  keywords: set(\"abort absolute access action active add admin after aggregate all also alter always analyse analyze and any array as asc assertion assignment asymmetric at authorization backward before begin between bigint binary bit boolean both by cache called cascade cascaded case cast chain char character characteristics check checkpoint class close cluster coalesce codegen collate column comment commit committed concurrency concurrently configuration connection constraint constraints contains content continue conversion copy cost cpu_rate_limit create createdb createexttable createrole createuser cross csv cube current current_catalog current_date current_role current_schema current_time current_timestamp current_user cursor cycle data database day deallocate dec decimal declare decode default defaults deferrable deferred definer delete delimiter delimiters deny desc dictionary disable discard distinct distributed do document domain double drop dxl each else enable encoding encrypted end enum errors escape every except exchange exclude excluding exclusive execute exists explain extension external extract false family fetch fields filespace fill filter first float following for force foreign format forward freeze from full function global grant granted greatest group group_id grouping handler hash having header hold host hour identity if ignore ilike immediate immutable implicit in including inclusive increment index indexes inherit inherits initially inline inner inout input insensitive insert instead int integer intersect interval into invoker is isnull isolation join key language large last leading least left level like limit list listen load local localtime localtimestamp location lock log login mapping master match maxvalue median merge minute minvalue missing mode modifies modify month move name names national natural nchar new newline next no nocreatedb nocreateexttable nocreaterole nocreateuser noinherit nologin none noovercommit nosuperuser not nothing notify notnull nowait null nullif nulls numeric object of off offset oids old on only operator option options or order ordered others out outer over overcommit overlaps overlay owned owner parser partial partition partitions passing password percent percentile_cont percentile_disc placing plans position preceding precision prepare prepared preserve primary prior privileges procedural procedure protocol queue quote randomly range read readable reads real reassign recheck recursive ref references reindex reject relative release rename repeatable replace replica reset resource restart restrict returning returns revoke right role rollback rollup rootpartition row rows rule savepoint scatter schema scroll search second security segment select sequence serializable session session_user set setof sets share show similar simple smallint some split sql stable standalone start statement statistics stdin stdout storage strict strip subpartition subpartitions substring superuser symmetric sysid system table tablespace temp template temporary text then threshold ties time timestamp to trailing transaction treat trigger trim true truncate trusted type unbounded uncommitted unencrypted union unique unknown unlisten until update user using vacuum valid validation validator value values varchar variadic varying verbose version view volatile web when where whitespace window with within without work writable write xml xmlattributes xmlconcat xmlelement xmlexists xmlforest xmlparse xmlpi xmlroot xmlserialize year yes zone\"),\n  builtin: set(\"bigint int8 bigserial serial8 bit varying varbit boolean bool box bytea character char varchar cidr circle date double precision float float8 inet integer int int4 interval json jsonb line lseg macaddr macaddr8 money numeric decimal path pg_lsn point polygon real float4 smallint int2 smallserial serial2 serial serial4 text time without zone with timetz timestamp timestamptz tsquery tsvector txid_snapshot uuid xml\"),\n  atoms: set(\"false true null unknown\"),\n  operatorChars: /^[*+\\-%<>!=&|^\\/#@?~]/,\n  dateSQL: set(\"date time timestamp\"),\n  support: set(\"ODBCdotTable decimallessFloat zerolessFloat binaryNumber hexNumber nCharCast charsetCast\")\n});\n\n// Spark SQL\nexport const sparkSQL = sql({\n  keywords: set(\"add after all alter analyze and anti archive array as asc at between bucket buckets by cache cascade case cast change clear cluster clustered codegen collection column columns comment commit compact compactions compute concatenate cost create cross cube current current_date current_timestamp database databases data dbproperties defined delete delimited deny desc describe dfs directories distinct distribute drop else end escaped except exchange exists explain export extended external false fields fileformat first following for format formatted from full function functions global grant group grouping having if ignore import in index indexes inner inpath inputformat insert intersect interval into is items join keys last lateral lazy left like limit lines list load local location lock locks logical macro map minus msck natural no not null nulls of on optimize option options or order out outer outputformat over overwrite partition partitioned partitions percent preceding principals purge range recordreader recordwriter recover reduce refresh regexp rename repair replace reset restrict revoke right rlike role roles rollback rollup row rows schema schemas select semi separated serde serdeproperties set sets show skewed sort sorted start statistics stored stratify struct table tables tablesample tblproperties temp temporary terminated then to touch transaction transactions transform true truncate unarchive unbounded uncache union unlock unset use using values view when where window with\"),\n  builtin: set(\"tinyint smallint int bigint boolean float double string binary timestamp decimal array map struct uniontype delimited serde sequencefile textfile rcfile inputformat outputformat\"),\n  atoms: set(\"false true null\"),\n  operatorChars: /^[*\\/+\\-%<>!=~&|^]/,\n  dateSQL: set(\"date time timestamp\"),\n  support: set(\"ODBCdotTable doubleQuote zerolessFloat\")\n});\n\n// Esper\nexport const esper = sql({\n  client: set(\"source\"),\n  // http://www.espertech.com/esper/release-5.5.0/esper-reference/html/appendix_keywords.html\n  keywords: set(\"alter and as asc between by count create delete desc distinct drop from group having in insert into is join like not on or order select set table union update values where limit after all and as at asc avedev avg between by case cast coalesce count create current_timestamp day days delete define desc distinct else end escape events every exists false first from full group having hour hours in inner insert instanceof into irstream is istream join last lastweekday left limit like max match_recognize matches median measures metadatasql min minute minutes msec millisecond milliseconds not null offset on or order outer output partition pattern prev prior regexp retain-union retain-intersection right rstream sec second seconds select set some snapshot sql stddev sum then true unidirectional until update variable weekday when where window\"),\n  builtin: {},\n  atoms: set(\"false true null\"),\n  operatorChars: /^[*+\\-%<>!=&|^\\/#@?~]/,\n  dateSQL: set(\"time\"),\n  support: set(\"decimallessFloat zerolessFloat binaryNumber hexNumber\")\n});\n\n/*\n  How options are used by SQL Mode\n  =================================================\n\n  keywords:\n    A list of keywords you want to be highlighted.\n  builtin:\n    A list of builtin types you want to be highlighted (if you want types to be of class \"builtin\" instead of \"keyword\").\n  operatorChars:\n    All characters that must be handled as operators.\n  client:\n    Commands parsed and executed by the client (not the server).\n  support:\n    A list of supported syntaxes which are not common, but are supported by more than 1 DBMS.\n    * ODBCdotTable: .tableName\n    * zerolessFloat: .1\n    * doubleQuote\n    * nCharCast: N'string'\n    * charsetCast: _utf8'string'\n    * commentHash: use # char for comments\n    * commentSlashSlash: use // for comments\n    * commentSpaceRequired: require a space after -- for comments\n  atoms:\n    Keywords that must be highlighted as atoms,. Some DBMS's support more atoms than others:\n    UNKNOWN, INFINITY, UNDERFLOW, NaN...\n  dateSQL:\n    Used for date/time SQL standard syntax, because not all DBMS's support same temporal types.\n*/\n"],"names":[],"sourceRoot":""}
./.venv/lib/python3.13/site-packages/notebook/static/8479.1807152edb3d746c4d0b.js:424:  // For pl/pgsql lang - https://github.com/postgres/postgres/blob/REL_11_2/src/pl/plpgsql/src/pl_scanner.c
./.venv/lib/python3.13/site-packages/notebook/static/632.c59cde46a58f6dac3b70.js.map:1:{"version":3,"file":"632.c59cde46a58f6dac3b70.js?v=c59cde46a58f6dac3b70","mappings":";;;;;;;;;;AAAA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA,sBAAsB;AACtB;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,gDAAgD;AAChD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEO;AACP;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;;AAEA;AACA,GAAG;;AAEH;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA,GAAG;;AAEH;AACA;AACA,oBAAoB,mBAAmB;AACvC;AACA","sources":["webpack://_JUPYTERLAB.CORE_OUTPUT/../node_modules/@codemirror/legacy-modes/mode/oz.js"],"sourcesContent":["function wordRegexp(words) {\n  return new RegExp(\"^((\" + words.join(\")|(\") + \"))\\\\b\");\n}\n\nvar singleOperators = /[\\^@!\\|<>#~\\.\\*\\-\\+\\\\/,=]/;\nvar doubleOperators = /(<-)|(:=)|(=<)|(>=)|(<=)|(<:)|(>:)|(=:)|(\\\\=)|(\\\\=:)|(!!)|(==)|(::)/;\nvar tripleOperators = /(:::)|(\\.\\.\\.)|(=<:)|(>=:)/;\n\nvar middle = [\"in\", \"then\", \"else\", \"of\", \"elseof\", \"elsecase\", \"elseif\", \"catch\",\n              \"finally\", \"with\", \"require\", \"prepare\", \"import\", \"export\", \"define\", \"do\"];\nvar end = [\"end\"];\n\nvar atoms = wordRegexp([\"true\", \"false\", \"nil\", \"unit\"]);\nvar commonKeywords = wordRegexp([\"andthen\", \"at\", \"attr\", \"declare\", \"feat\", \"from\", \"lex\",\n                                 \"mod\", \"div\", \"mode\", \"orelse\", \"parser\", \"prod\", \"prop\", \"scanner\", \"self\", \"syn\", \"token\"]);\nvar openingKeywords = wordRegexp([\"local\", \"proc\", \"fun\", \"case\", \"class\", \"if\", \"cond\", \"or\", \"dis\",\n                                  \"choice\", \"not\", \"thread\", \"try\", \"raise\", \"lock\", \"for\", \"suchthat\", \"meth\", \"functor\"]);\nvar middleKeywords = wordRegexp(middle);\nvar endKeywords = wordRegexp(end);\n\n// Tokenizers\nfunction tokenBase(stream, state) {\n  if (stream.eatSpace()) {\n    return null;\n  }\n\n  // Brackets\n  if(stream.match(/[{}]/)) {\n    return \"bracket\";\n  }\n\n  // Special [] keyword\n  if (stream.match('[]')) {\n    return \"keyword\"\n  }\n\n  // Operators\n  if (stream.match(tripleOperators) || stream.match(doubleOperators)) {\n    return \"operator\";\n  }\n\n  // Atoms\n  if(stream.match(atoms)) {\n    return 'atom';\n  }\n\n  // Opening keywords\n  var matched = stream.match(openingKeywords);\n  if (matched) {\n    if (!state.doInCurrentLine)\n      state.currentIndent++;\n    else\n      state.doInCurrentLine = false;\n\n    // Special matching for signatures\n    if(matched[0] == \"proc\" || matched[0] == \"fun\")\n      state.tokenize = tokenFunProc;\n    else if(matched[0] == \"class\")\n      state.tokenize = tokenClass;\n    else if(matched[0] == \"meth\")\n      state.tokenize = tokenMeth;\n\n    return 'keyword';\n  }\n\n  // Middle and other keywords\n  if (stream.match(middleKeywords) || stream.match(commonKeywords)) {\n    return \"keyword\"\n  }\n\n  // End keywords\n  if (stream.match(endKeywords)) {\n    state.currentIndent--;\n    return 'keyword';\n  }\n\n  // Eat the next char for next comparisons\n  var ch = stream.next();\n\n  // Strings\n  if (ch == '\"' || ch == \"'\") {\n    state.tokenize = tokenString(ch);\n    return state.tokenize(stream, state);\n  }\n\n  // Numbers\n  if (/[~\\d]/.test(ch)) {\n    if (ch == \"~\") {\n      if(! /^[0-9]/.test(stream.peek()))\n        return null;\n      else if (( stream.next() == \"0\" && stream.match(/^[xX][0-9a-fA-F]+/)) || stream.match(/^[0-9]*(\\.[0-9]+)?([eE][~+]?[0-9]+)?/))\n        return \"number\";\n    }\n\n    if ((ch == \"0\" && stream.match(/^[xX][0-9a-fA-F]+/)) || stream.match(/^[0-9]*(\\.[0-9]+)?([eE][~+]?[0-9]+)?/))\n      return \"number\";\n\n    return null;\n  }\n\n  // Comments\n  if (ch == \"%\") {\n    stream.skipToEnd();\n    return 'comment';\n  }\n  else if (ch == \"/\") {\n    if (stream.eat(\"*\")) {\n      state.tokenize = tokenComment;\n      return tokenComment(stream, state);\n    }\n  }\n\n  // Single operators\n  if(singleOperators.test(ch)) {\n    return \"operator\";\n  }\n\n  // If nothing match, we skip the entire alphanumerical block\n  stream.eatWhile(/\\w/);\n\n  return \"variable\";\n}\n\nfunction tokenClass(stream, state) {\n  if (stream.eatSpace()) {\n    return null;\n  }\n  stream.match(/([A-Z][A-Za-z0-9_]*)|(`.+`)/);\n  state.tokenize = tokenBase;\n  return \"type\"\n}\n\nfunction tokenMeth(stream, state) {\n  if (stream.eatSpace()) {\n    return null;\n  }\n  stream.match(/([a-zA-Z][A-Za-z0-9_]*)|(`.+`)/);\n  state.tokenize = tokenBase;\n  return \"def\"\n}\n\nfunction tokenFunProc(stream, state) {\n  if (stream.eatSpace()) {\n    return null;\n  }\n\n  if(!state.hasPassedFirstStage && stream.eat(\"{\")) {\n    state.hasPassedFirstStage = true;\n    return \"bracket\";\n  }\n  else if(state.hasPassedFirstStage) {\n    stream.match(/([A-Z][A-Za-z0-9_]*)|(`.+`)|\\$/);\n    state.hasPassedFirstStage = false;\n    state.tokenize = tokenBase;\n    return \"def\"\n  }\n  else {\n    state.tokenize = tokenBase;\n    return null;\n  }\n}\n\nfunction tokenComment(stream, state) {\n  var maybeEnd = false, ch;\n  while (ch = stream.next()) {\n    if (ch == \"/\" && maybeEnd) {\n      state.tokenize = tokenBase;\n      break;\n    }\n    maybeEnd = (ch == \"*\");\n  }\n  return \"comment\";\n}\n\nfunction tokenString(quote) {\n  return function (stream, state) {\n    var escaped = false, next, end = false;\n    while ((next = stream.next()) != null) {\n      if (next == quote && !escaped) {\n        end = true;\n        break;\n      }\n      escaped = !escaped && next == \"\\\\\";\n    }\n    if (end || !escaped)\n      state.tokenize = tokenBase;\n    return \"string\";\n  };\n}\n\nfunction buildElectricInputRegEx() {\n  // Reindentation should occur on [] or on a match of any of\n  // the block closing keywords, at the end of a line.\n  var allClosings = middle.concat(end);\n  return new RegExp(\"[\\\\[\\\\]]|(\" + allClosings.join(\"|\") + \")$\");\n}\n\nexport const oz = {\n  name: \"oz\",\n\n  startState: function () {\n    return {\n      tokenize: tokenBase,\n      currentIndent: 0,\n      doInCurrentLine: false,\n      hasPassedFirstStage: false\n    };\n  },\n\n  token: function (stream, state) {\n    if (stream.sol())\n      state.doInCurrentLine = 0;\n\n    return state.tokenize(stream, state);\n  },\n\n  indent: function (state, textAfter, cx) {\n    var trueText = textAfter.replace(/^\\s+|\\s+$/g, '');\n\n    if (trueText.match(endKeywords) || trueText.match(middleKeywords) || trueText.match(/(\\[])/))\n      return cx.unit * (state.currentIndent - 1);\n\n    if (state.currentIndent < 0)\n      return 0;\n\n    return state.currentIndent * cx.unit\n  },\n\n  languageData: {\n    indentOnInut: buildElectricInputRegEx(),\n    commentTokens: {line: \"%\", block: {open: \"/*\", close: \"*/\"}}\n  }\n};\n"],"names":[],"sourceRoot":""}
./.venv/lib/python3.13/site-packages/coverage/lcovreport.py:27:    # scanners raise alarms about the use of MD5 here, but it is a false
./.venv/share/jupyter/lab/static/7445.7c793c8e1720f8ec4f85.js:1:"use strict";(self["webpackChunk_jupyterlab_application_top"]=self["webpackChunk_jupyterlab_application_top"]||[]).push([[7445],{57445:(e,t,n)=>{n.r(t);n.d(t,{oz:()=>g});function r(e){return new RegExp("^(("+e.join(")|(")+"))\\b")}var a=/[\^@!\|<>#~\.\*\-\+\\/,=]/;var i=/(<-)|(:=)|(=<)|(>=)|(<=)|(<:)|(>:)|(=:)|(\\=)|(\\=:)|(!!)|(==)|(::)/;var u=/(:::)|(\.\.\.)|(=<:)|(>=:)/;var o=["in","then","else","of","elseof","elsecase","elseif","catch","finally","with","require","prepare","import","export","define","do"];var c=["end"];var f=r(["true","false","nil","unit"]);var s=r(["andthen","at","attr","declare","feat","from","lex","mod","div","mode","orelse","parser","prod","prop","scanner","self","syn","token"]);var l=r(["local","proc","fun","case","class","if","cond","or","dis","choice","not","thread","try","raise","lock","for","suchthat","meth","functor"]);var h=r(o);var d=r(c);function m(e,t){if(e.eatSpace()){return null}if(e.match(/[{}]/)){return"bracket"}if(e.match("[]")){return"keyword"}if(e.match(u)||e.match(i)){return"operator"}if(e.match(f)){return"atom"}var n=e.match(l);if(n){if(!t.doInCurrentLine)t.currentIndent++;else t.doInCurrentLine=false;if(n[0]=="proc"||n[0]=="fun")t.tokenize=z;else if(n[0]=="class")t.tokenize=p;else if(n[0]=="meth")t.tokenize=k;return"keyword"}if(e.match(h)||e.match(s)){return"keyword"}if(e.match(d)){t.currentIndent--;return"keyword"}var r=e.next();if(r=='"'||r=="'"){t.tokenize=b(r);return t.tokenize(e,t)}if(/[~\d]/.test(r)){if(r=="~"){if(!/^[0-9]/.test(e.peek()))return null;else if(e.next()=="0"&&e.match(/^[xX][0-9a-fA-F]+/)||e.match(/^[0-9]*(\.[0-9]+)?([eE][~+]?[0-9]+)?/))return"number"}if(r=="0"&&e.match(/^[xX][0-9a-fA-F]+/)||e.match(/^[0-9]*(\.[0-9]+)?([eE][~+]?[0-9]+)?/))return"number";return null}if(r=="%"){e.skipToEnd();return"comment"}else if(r=="/"){if(e.eat("*")){t.tokenize=v;return v(e,t)}}if(a.test(r)){return"operator"}e.eatWhile(/\w/);return"variable"}function p(e,t){if(e.eatSpace()){return null}e.match(/([A-Z][A-Za-z0-9_]*)|(`.+`)/);t.tokenize=m;return"type"}function k(e,t){if(e.eatSpace()){return null}e.match(/([a-zA-Z][A-Za-z0-9_]*)|(`.+`)/);t.tokenize=m;return"def"}function z(e,t){if(e.eatSpace()){return null}if(!t.hasPassedFirstStage&&e.eat("{")){t.hasPassedFirstStage=true;return"bracket"}else if(t.hasPassedFirstStage){e.match(/([A-Z][A-Za-z0-9_]*)|(`.+`)|\$/);t.hasPassedFirstStage=false;t.tokenize=m;return"def"}else{t.tokenize=m;return null}}function v(e,t){var n=false,r;while(r=e.next()){if(r=="/"&&n){t.tokenize=m;break}n=r=="*"}return"comment"}function b(e){return function(t,n){var r=false,a,i=false;while((a=t.next())!=null){if(a==e&&!r){i=true;break}r=!r&&a=="\\"}if(i||!r)n.tokenize=m;return"string"}}function w(){var e=o.concat(c);return new RegExp("[\\[\\]]|("+e.join("|")+")$")}const g={name:"oz",startState:function(){return{tokenize:m,currentIndent:0,doInCurrentLine:false,hasPassedFirstStage:false}},token:function(e,t){if(e.sol())t.doInCurrentLine=0;return t.tokenize(e,t)},indent:function(e,t,n){var r=t.replace(/^\s+|\s+$/g,"");if(r.match(d)||r.match(h)||r.match(/(\[])/))return n.unit*(e.currentIndent-1);if(e.currentIndent<0)return 0;return e.currentIndent*n.unit},languageData:{indentOnInut:w(),commentTokens:{line:"%",block:{open:"/*",close:"*/"}}}}}}]);
Binary file ./.git/index matches
./main.py:164:                bot.scanner.update_enabled_symbols(config.symbols)
./services/unified_trading/__init__.py:17:    refresh_scanner_symbols,
./services/unified_trading/__init__.py:19:    to_scanner_symbol,
./services/unified_trading/__init__.py:34:    "refresh_scanner_symbols",
./services/unified_trading/__init__.py:36:    "to_scanner_symbol",
./services/unified_trading/state.py:18:def to_scanner_symbol(symbol: str) -> str:
./services/unified_trading/state.py:19:    """Convert display symbols (e.g. ``BTC/USD``) to scanner friendly format."""
./services/unified_trading/state.py:25:    """Convert scanner symbols back to Alpaca display format."""
./services/unified_trading/state.py:105:    crypto_scanner: Any = None
./services/unified_trading/state.py:110:    crypto_scanner_symbols: List[str] = field(default_factory=list)
./services/unified_trading/state.py:125:def refresh_scanner_symbols(state: TradingState) -> List[str]:
./services/unified_trading/state.py:126:    """Synchronise cached scanner symbols with the active strategy."""
./services/unified_trading/state.py:128:    if not state.crypto_scanner:
./services/unified_trading/state.py:129:        return state.crypto_scanner_symbols
./services/unified_trading/state.py:131:    enabled_symbols = state.crypto_scanner.get_enabled_symbols()
./services/unified_trading/state.py:133:        state.crypto_scanner_symbols = []
./services/unified_trading/state.py:134:        return state.crypto_scanner_symbols
./services/unified_trading/state.py:136:    state.crypto_scanner_symbols = sorted(
./services/unified_trading/state.py:139:    return state.crypto_scanner_symbols
./services/unified_trading/state.py:186:    "refresh_scanner_symbols",
./services/unified_trading/state.py:188:    "to_scanner_symbol",
